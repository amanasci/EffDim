{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Welcome to EffDim","text":"<p>EffDim is a unified, research-oriented Python library designed to compute \"effective dimensionality\" (ED) across diverse data modalities.</p> <p>It aims to standardize the fragmented landscape of ED metrics found in statistics, physics, information theory, and machine learning into a single, cohesive interface.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Modality Agnostic: Works robustly across different datasets.</li> <li>Unified Interface: Simple <code>compute_dim</code> function to get all estimates.</li> <li>Extensive Estimators: PCA, Participation Ratio, Shannon Entropy, and more.</li> <li>Research Ready: Accurate implementations of metrics from literature.</li> </ul>"},{"location":"#installation","title":"Installation","text":"<p>Install via pip:</p> <pre><code>pip install effdim\n</code></pre> <p>(EffDim relies on Faiss for fast kNN approximation under the hood).</p>"},{"location":"#quick-start","title":"Quick Start","text":"<pre><code>import numpy as np\nimport effdim\n\n# Generate random high-dimensional data\ndata = np.random.randn(100, 50)\n\n# Compute all Effective Dimensions at once\nresults = effdim.compute_dim(data)\n\n# Extract specific metrics\ned = results['pca_explained_variance_95']\nprint(f\"Effective Dimension (PCA): {ed}\")\n\npr = results['participation_ratio']\nprint(f\"Participation Ratio: {pr}\")\n</code></pre> <p>Explore the User Guide for more examples.</p>"},{"location":"api/","title":"API Reference","text":""},{"location":"api/#main-interface","title":"Main Interface","text":""},{"location":"api/#effdim.api.compute_dim","title":"<code>compute_dim(data)</code>","text":"<p>Compute the effective dimensionality of the given data using the specified method.</p>"},{"location":"api/#effdim.api.compute_dim--parameters","title":"Parameters:","text":"<p>data : Union[np.ndarray, List[np.ndarray]]     Input data. Can be a single numpy array or a list of numpy arrays. Returns: dict     A dictionary containing the results of the effective dimensionality computation.</p> Source code in <code>src/effdim/api.py</code> <pre><code>def compute_dim(data: Union[np.ndarray, List[np.ndarray]]) -&gt; Dict[str, Any]:\n    \"\"\"\n    Compute the effective dimensionality of the given data using the specified method.\n\n    Parameters:\n    -----------\n    data : Union[np.ndarray, List[np.ndarray]]\n        Input data. Can be a single numpy array or a list of numpy arrays.\n    Returns: dict\n        A dictionary containing the results of the effective dimensionality computation.\n    \"\"\"\n    results: Dict[str, Any] = {}\n\n    # Getting the data and then converting to numpy array if it's a list\n    if isinstance(data, list):\n        data = np.vstack(data)\n    elif not isinstance(data, np.ndarray):\n        raise ValueError(\"Input data must be a numpy array or a list of numpy arrays.\")\n\n    # Ensure the data is centered\n    data = _ensure_centered(data)\n    s = _do_svd(data)\n\n    # gettinf the eigenvalues from the singular values for the covariance matrix\n    eigenvalues = (s**2) / (data.shape[0] - 1)\n\n    # Total variance\n    total_variance = np.sum(eigenvalues)\n\n    #  getting the probabilities\n    if total_variance == 0:\n        probabilities = np.zeros_like(eigenvalues)\n    else:\n        probabilities = eigenvalues / total_variance\n\n    # Computing various effective dimensionalities\n    results[\"pca_explained_variance_95\"] = pca_explained_variance(\n        eigenvalues, threshold=0.95\n    )\n    results[\"participation_ratio\"] = participation_ratio(eigenvalues)\n    results[\"shannon_entropy\"] = shannon_entropy(probabilities)\n\n    # Renyi effective dimensionalities for alpha = 2,3,4,5\n    for i in range(2, 6):\n        results[f\"renyi_eff_dimensionality_alpha_{i}\"] = renyi_eff_dimensionality(\n            probabilities, alpha=i\n        )\n\n    # Geometric Dimensions\n    results[\"geometric_mean_eff_dimensionality\"] = geometric_mean_eff_dimensionality(\n        probabilities\n    )\n\n    # Compute KNN distances once for the largest k needed (MLE uses k=10 by default)\n    # We use k=10 as a safe upper bound for default usage.\n    # Convert data to float32 contiguous array once for geometry functions\n    data_f32 = np.ascontiguousarray(data, dtype=np.float32)\n\n    knn_dist_sq = compute_knn_distances(data_f32, k=10)\n\n    results[\"mle_dimensionality\"] = mle_dimensionality(\n        data_f32, precomputed_knn_dist_sq=knn_dist_sq\n    )\n    results[\"two_nn_dimensionality\"] = two_nn_dimensionality(\n        data_f32, precomputed_knn_dist_sq=knn_dist_sq\n    )\n    results[\"danco_dimensionality\"] = danco_dimensionality(\n        data_f32, precomputed_knn_dist_sq=knn_dist_sq\n    )\n    results[\"mind_mli_dimensionality\"] = mind_mli_dimensionality(\n        data_f32, precomputed_knn_dist_sq=knn_dist_sq\n    )\n    results[\"mind_mlk_dimensionality\"] = mind_mlk_dimensionality(\n        data_f32, precomputed_knn_dist_sq=knn_dist_sq\n    )\n    results[\"ess_dimensionality\"] = ess_dimensionality(\n        data_f32, precomputed_knn_dist_sq=knn_dist_sq\n    )\n    results[\"tle_dimensionality\"] = tle_dimensionality(\n        data_f32, precomputed_knn_dist_sq=knn_dist_sq\n    )\n    results[\"gmst_dimensionality\"] = gmst_dimensionality(data_f32)\n\n    return results\n</code></pre>"},{"location":"api/#metrics-spectral","title":"Metrics (Spectral)","text":""},{"location":"api/#effdim.metrics.geometric_mean_eff_dimensionality","title":"<code>geometric_mean_eff_dimensionality(spectrum)</code>","text":"<p>Compute the Geometric Mean Effective Dimensionality of the given spectrum.</p>"},{"location":"api/#effdim.metrics.geometric_mean_eff_dimensionality--parameters","title":"Parameters:","text":"<p>spectrum : np.ndarray     Array of eigenvalues.</p>"},{"location":"api/#effdim.metrics.geometric_mean_eff_dimensionality--returns","title":"Returns:","text":"<p>float     Geometric Mean Effective Dimensionality value.</p> Source code in <code>src/effdim/metrics.py</code> <pre><code>def geometric_mean_eff_dimensionality(spectrum: np.ndarray) -&gt; float:\n    \"\"\"\n    Compute the Geometric Mean Effective Dimensionality of the given spectrum.\n\n    Parameters:\n    -----------\n    spectrum : np.ndarray\n        Array of eigenvalues.\n\n    Returns:\n    --------\n    float\n        Geometric Mean Effective Dimensionality value.\n    \"\"\"\n    positive_spectrum = spectrum[spectrum &gt; 0]\n    if len(positive_spectrum) == 0:\n        return 0.0\n\n    # Calculate the arithmetic mean of the positive spectrum\n    am = np.mean(positive_spectrum)\n    # Calculate the geometric mean of the positive spectrum\n    gm = np.exp(np.mean(np.log(positive_spectrum)))\n    d_eff = (am / gm)\n\n    return d_eff\n</code></pre>"},{"location":"api/#effdim.metrics.participation_ratio","title":"<code>participation_ratio(spectrum)</code>","text":"<p>Compute the Participation Ratio (PR) of the given spectrum.</p>"},{"location":"api/#effdim.metrics.participation_ratio--parameters","title":"Parameters:","text":"<p>spectrum : np.ndarray     Array of eigenvalues.</p>"},{"location":"api/#effdim.metrics.participation_ratio--returns","title":"Returns:","text":"<p>float     Participation Ratio value.</p> Source code in <code>src/effdim/metrics.py</code> <pre><code>def participation_ratio(spectrum: np.ndarray) -&gt; float:\n    \"\"\"\n    Compute the Participation Ratio (PR) of the given spectrum.\n\n    Parameters:\n    -----------\n    spectrum : np.ndarray\n        Array of eigenvalues.\n\n    Returns:\n    --------\n    float\n        Participation Ratio value.\n    \"\"\"\n    numerator = (np.sum(spectrum)) ** 2\n    denominator = np.sum(spectrum ** 2)\n    if denominator == 0:\n        return 0.0\n    return numerator / denominator\n</code></pre>"},{"location":"api/#effdim.metrics.pca_explained_variance","title":"<code>pca_explained_variance(spectrum, threshold=0.95)</code>","text":"<p>Compute the number of principal components required to explain a given threshold of variance.</p>"},{"location":"api/#effdim.metrics.pca_explained_variance--parameters","title":"Parameters:","text":"<p>spectrum : np.ndarray     Array of eigenvalues (explained variance) from PCA. threshold : float     The cumulative variance threshold to reach (between 0 and 1).</p>"},{"location":"api/#effdim.metrics.pca_explained_variance--returns","title":"Returns:","text":"<p>int     Number of principal components needed to reach the threshold.</p> Source code in <code>src/effdim/metrics.py</code> <pre><code>def pca_explained_variance(spectrum: np.ndarray, threshold: float = 0.95) -&gt; int:\n    \"\"\"\n    Compute the number of principal components required to explain a given\n    threshold of variance.\n\n    Parameters:\n    -----------\n    spectrum : np.ndarray\n        Array of eigenvalues (explained variance) from PCA.\n    threshold : float\n        The cumulative variance threshold to reach (between 0 and 1).\n\n    Returns:\n    --------\n    int\n        Number of principal components needed to reach the threshold.\n    \"\"\"\n    total_variance = np.sum(spectrum)\n    cumulative_variance = np.cumsum(spectrum)\n    explained_variance_ratio = cumulative_variance / total_variance\n\n    num_components = int(np.searchsorted(explained_variance_ratio, threshold) + 1)\n    return num_components\n</code></pre>"},{"location":"api/#effdim.metrics.renyi_eff_dimensionality","title":"<code>renyi_eff_dimensionality(probabilities, alpha)</code>","text":"<p>Compute the R\u00e9nyi Effective Dimensionality of the given probability distribution.</p>"},{"location":"api/#effdim.metrics.renyi_eff_dimensionality--parameters","title":"Parameters:","text":"<p>probabilities : np.ndarray     Array of probabilities. alpha : float     Order of the R\u00e9nyi entropy (alpha &gt; 0 and alpha != 1).</p>"},{"location":"api/#effdim.metrics.renyi_eff_dimensionality--returns","title":"Returns:","text":"<p>float     R\u00e9nyi Effective Dimensionality value.</p> Source code in <code>src/effdim/metrics.py</code> <pre><code>def renyi_eff_dimensionality(probabilities: np.ndarray, alpha: float) -&gt; float:\n    \"\"\"\n    Compute the R\u00e9nyi Effective Dimensionality of the given probability distribution.\n\n    Parameters:\n    -----------\n    probabilities : np.ndarray\n        Array of probabilities.\n    alpha : float\n        Order of the R\u00e9nyi entropy (alpha &gt; 0 and alpha != 1).\n\n    Returns:\n    --------\n    float\n        R\u00e9nyi Effective Dimensionality value.\n    \"\"\"\n    if alpha &lt;= 0 or alpha == 1:\n        raise ValueError(\"Alpha must be greater than 0 and not equal to 1.\")\n\n    sum_probs_alpha = np.sum(probabilities ** alpha)\n    if sum_probs_alpha == 0:\n        return 0.0\n\n    d_eff = sum_probs_alpha ** (1 / (1 - alpha))\n    return d_eff\n</code></pre>"},{"location":"api/#effdim.metrics.shannon_entropy","title":"<code>shannon_entropy(probabilities)</code>","text":"<p>Compute the Shannon Entropy of the given probability distribution.</p>"},{"location":"api/#effdim.metrics.shannon_entropy--parameters","title":"Parameters:","text":"<p>probabilities : np.ndarray     Array of probabilities.</p>"},{"location":"api/#effdim.metrics.shannon_entropy--returns","title":"Returns:","text":"<p>float     Shannon Entropy value.</p> Source code in <code>src/effdim/metrics.py</code> <pre><code>def shannon_entropy(probabilities: np.ndarray) -&gt; float:\n    \"\"\"\n    Compute the Shannon Entropy of the given probability distribution.\n\n    Parameters:\n    -----------\n    probabilities : np.ndarray\n        Array of probabilities.\n\n    Returns:\n    --------\n    float\n        Shannon Entropy value.\n    \"\"\"\n    # Filter out zero probabilities to avoid log(0)\n    probabilities = probabilities[probabilities &gt; 0]\n    entropy = -np.sum(probabilities * np.log(probabilities))\n    d_eff = np.exp(entropy)\n    return d_eff\n</code></pre>"},{"location":"api/#geometry-spatial","title":"Geometry (Spatial)","text":""},{"location":"api/#effdim.geometry.compute_knn_distances","title":"<code>compute_knn_distances(data, k)</code>","text":"<p>Compute k nearest neighbors distances for each point in data. Returns squared distances. Excludes the point itself (distance 0).</p> Source code in <code>src/effdim/geometry.py</code> <pre><code>def compute_knn_distances(data: np.ndarray, k: int) -&gt; np.ndarray:\n    \"\"\"\n    Compute k nearest neighbors distances for each point in data.\n    Returns squared distances.\n    Excludes the point itself (distance 0).\n    \"\"\"\n    n_samples, n_features = data.shape\n    # Ensure data is float32 for FAISS and contiguous\n    data = np.ascontiguousarray(data, dtype=np.float32)\n\n    # Exact search using L2 distance\n    index = faiss.IndexFlatL2(n_features)\n    index.add(data)\n\n    # Search for k+1 neighbors (including self)\n    k_search = min(k + 1, n_samples)\n\n    # distances_sq is (n_samples, k_search)\n    # indices is (n_samples, k_search)\n    distances_sq, _ = index.search(data, k_search)\n\n    # The first column corresponds to the point itself (distance ~0)\n    # We return the remaining k columns\n    return distances_sq[:, 1:]\n</code></pre>"},{"location":"api/#effdim.geometry.danco_dimensionality","title":"<code>danco_dimensionality(data, k=10, precomputed_knn_dist_sq=None)</code>","text":"<p>Estimate intrinsic dimensionality using DANCo (Dimensionality from Angle and Norm Concentration). Exploits the concentration of angles between nearest neighbor vectors. Uses FAISS for fast nearest neighbor search.</p> Source code in <code>src/effdim/geometry.py</code> <pre><code>def danco_dimensionality(\n    data: np.ndarray,\n    k: int = 10,\n    precomputed_knn_dist_sq: Optional[np.ndarray] = None\n) -&gt; float:\n    \"\"\"\n    Estimate intrinsic dimensionality using DANCo\n    (Dimensionality from Angle and Norm Concentration).\n    Exploits the concentration of angles between nearest neighbor vectors.\n    Uses FAISS for fast nearest neighbor search.\n    \"\"\"\n    n_samples, n_features = data.shape\n    if n_samples &lt; 3:\n        return 0.0\n\n    if precomputed_knn_dist_sq is not None:\n        k = precomputed_knn_dist_sq.shape[1]\n\n    k = min(k, n_samples - 1)\n    if k &lt; 2:\n        return 0.0\n\n    # Build FAISS index for neighbor indices\n    data_f32 = np.ascontiguousarray(data, dtype=np.float32)\n    index = faiss.IndexFlatL2(n_features)\n    index.add(data_f32)\n    k_search = min(k + 1, n_samples)\n    _, indices = index.search(data_f32, k_search)\n\n    neighbor_indices = indices[:, 1:]\n    k_actual = neighbor_indices.shape[1]\n\n    if k_actual &lt; 2:\n        return 0.0\n\n    # Compute vectors from each point to its neighbors\n    vectors = data[neighbor_indices] - data[:, np.newaxis, :]\n\n    # Normalize to unit vectors\n    norms = np.linalg.norm(vectors, axis=2, keepdims=True) + 1e-10\n    unit_vectors = vectors / norms\n\n    # Compute pairwise cosines for each point\n    cos_matrix = np.einsum('nik,njk-&gt;nij', unit_vectors, unit_vectors)\n\n    # Extract upper triangle (excluding diagonal) for each point\n    triu_idx = np.triu_indices(k_actual, k=1)\n    cos_vals = cos_matrix[:, triu_idx[0], triu_idx[1]]\n\n    mean_cos_sq = np.mean(cos_vals ** 2)\n    if mean_cos_sq &lt; 1e-10:\n        return 0.0\n\n    return float(1.0 / mean_cos_sq)\n</code></pre>"},{"location":"api/#effdim.geometry.ess_dimensionality","title":"<code>ess_dimensionality(data, k=10, precomputed_knn_dist_sq=None)</code>","text":"<p>Estimate intrinsic dimensionality using ESS (Expected Simplex Skewness). Analyzes the skewness of local simplices formed by nearest neighbors. Uses FAISS for fast nearest neighbor search.</p> Source code in <code>src/effdim/geometry.py</code> <pre><code>def ess_dimensionality(\n    data: np.ndarray,\n    k: int = 10,\n    precomputed_knn_dist_sq: Optional[np.ndarray] = None\n) -&gt; float:\n    \"\"\"\n    Estimate intrinsic dimensionality using ESS\n    (Expected Simplex Skewness).\n    Analyzes the skewness of local simplices formed by nearest neighbors.\n    Uses FAISS for fast nearest neighbor search.\n    \"\"\"\n    n_samples, n_features = data.shape\n    if n_samples &lt; 3:\n        return 0.0\n\n    if precomputed_knn_dist_sq is not None:\n        k = precomputed_knn_dist_sq.shape[1]\n\n    k = min(k, n_samples - 1)\n    if k &lt; 1:\n        return 0.0\n\n    # Build FAISS index for neighbor indices\n    data_f32 = np.ascontiguousarray(data, dtype=np.float32)\n    index = faiss.IndexFlatL2(n_features)\n    index.add(data_f32)\n    k_search = min(k + 1, n_samples)\n    _, indices = index.search(data_f32, k_search)\n\n    neighbor_indices = indices[:, 1:]\n    k_actual = neighbor_indices.shape[1]\n\n    if k_actual &lt; 1:\n        return 0.0\n\n    # Compute vectors from each point to its neighbors\n    vectors = data[neighbor_indices] - data[:, np.newaxis, :]\n\n    # Normalize to unit vectors\n    norms = np.linalg.norm(vectors, axis=2, keepdims=True) + 1e-10\n    unit_vectors = vectors / norms\n\n    # Compute mean of unit vectors for each point\n    centroid = np.mean(unit_vectors, axis=1)\n\n    # Squared norm of centroid\n    S = np.sum(centroid ** 2, axis=1)\n    S_avg = np.mean(S)\n\n    if S_avg &lt; 1e-10:\n        return 0.0\n\n    return float(1.0 / (k_actual * S_avg))\n</code></pre>"},{"location":"api/#effdim.geometry.gmst_dimensionality","title":"<code>gmst_dimensionality(data, geodesic=False, random_state=42)</code>","text":"<p>Estimate intrinsic dimensionality using GMST (Geodesic Minimum Spanning Tree). Estimates dimension from the scaling of MST length with sample size.</p> Source code in <code>src/effdim/geometry.py</code> <pre><code>def gmst_dimensionality(\n    data: np.ndarray,\n    geodesic: bool = False,\n    random_state: int = 42\n) -&gt; float:\n    \"\"\"\n    Estimate intrinsic dimensionality using GMST\n    (Geodesic Minimum Spanning Tree).\n    Estimates dimension from the scaling of MST length with sample size.\n    \"\"\"\n    n_samples = data.shape[0]\n    if n_samples &lt; 10:\n        return 0.0\n\n    # Subsample sizes\n    sizes = sorted(set([\n        max(4, n_samples // 8),\n        max(4, n_samples // 4),\n        max(4, n_samples // 2),\n        n_samples\n    ]))\n\n    if len(sizes) &lt; 2:\n        return 0.0\n\n    rng = np.random.RandomState(random_state)\n    log_n_list = []\n    log_L_list = []\n\n    for size in sizes:\n        size = min(size, n_samples)\n        if size == n_samples:\n            idx = np.arange(n_samples)\n        else:\n            idx = rng.choice(n_samples, size=size, replace=False)\n\n        subsample = data[idx]\n\n        if geodesic:\n            k_geo = min(10, size - 1)\n            graph = kneighbors_graph(subsample, k_geo, mode='distance')\n            graph = graph + graph.T\n            dist_matrix = shortest_path(graph, directed=False)\n            finite_mask = np.isfinite(dist_matrix)\n            if not np.all(finite_mask):\n                max_dist = np.max(dist_matrix[finite_mask]) if np.any(finite_mask) else 1.0\n                dist_matrix[~finite_mask] = max_dist * 10\n        else:\n            dist_matrix = squareform(pdist(subsample))\n\n        mst = minimum_spanning_tree(dist_matrix)\n        L = mst.sum()\n\n        if L &gt; 0:\n            log_n_list.append(np.log(size))\n            log_L_list.append(np.log(L))\n\n    if len(log_n_list) &lt; 2:\n        return 0.0\n\n    # Linear regression: ln(L) = alpha * ln(n) + c\n    log_n_arr = np.array(log_n_list)\n    log_L_arr = np.array(log_L_list)\n\n    mean_x = np.mean(log_n_arr)\n    mean_y = np.mean(log_L_arr)\n\n    alpha = np.sum((log_n_arr - mean_x) * (log_L_arr - mean_y)) / (\n        np.sum((log_n_arr - mean_x) ** 2) + 1e-10\n    )\n\n    # d = 1 / (1 - alpha)\n    if abs(1.0 - alpha) &lt; 1e-10:\n        return 0.0\n\n    return float(1.0 / (1.0 - alpha))\n</code></pre>"},{"location":"api/#effdim.geometry.mind_mli_dimensionality","title":"<code>mind_mli_dimensionality(data, precomputed_knn_dist_sq=None)</code>","text":"<p>Estimate intrinsic dimensionality using MiND-MLi (Maximum Likelihood on Minimum Distances, single neighbor). Uses the distribution of nearest neighbor distances. Uses FAISS for fast nearest neighbor search.</p> Source code in <code>src/effdim/geometry.py</code> <pre><code>def mind_mli_dimensionality(\n    data: np.ndarray,\n    precomputed_knn_dist_sq: Optional[np.ndarray] = None\n) -&gt; float:\n    \"\"\"\n    Estimate intrinsic dimensionality using MiND-MLi\n    (Maximum Likelihood on Minimum Distances, single neighbor).\n    Uses the distribution of nearest neighbor distances.\n    Uses FAISS for fast nearest neighbor search.\n    \"\"\"\n    n_samples = data.shape[0]\n    if n_samples &lt; 3:\n        return 0.0\n\n    if precomputed_knn_dist_sq is None:\n        dist_sq = compute_knn_distances(data, 1)\n    else:\n        dist_sq = precomputed_knn_dist_sq\n\n    # Convert to Euclidean distances (nearest neighbor only)\n    dist = np.sqrt(dist_sq[:, 0]) + 1e-10\n\n    r_max = np.max(dist)\n\n    # Check if all distances are equal\n    if np.all(np.abs(dist - dist[0]) &lt; 1e-10):\n        return 0.0\n\n    # d = n / \u03a3\u1d62 ln(r_max / r\u1d62)\n    log_ratios = np.log(r_max / dist)\n    sum_log_ratios = np.sum(log_ratios)\n\n    if sum_log_ratios &lt; 1e-10:\n        return 0.0\n\n    return float(n_samples / sum_log_ratios)\n</code></pre>"},{"location":"api/#effdim.geometry.mind_mlk_dimensionality","title":"<code>mind_mlk_dimensionality(data, k=10, precomputed_knn_dist_sq=None)</code>","text":"<p>Estimate intrinsic dimensionality using MiND-MLk (Maximum Likelihood on Minimum Distances, k neighbors). Returns the median of per-point estimates for robustness. Uses FAISS for fast nearest neighbor search.</p> Source code in <code>src/effdim/geometry.py</code> <pre><code>def mind_mlk_dimensionality(\n    data: np.ndarray,\n    k: int = 10,\n    precomputed_knn_dist_sq: Optional[np.ndarray] = None\n) -&gt; float:\n    \"\"\"\n    Estimate intrinsic dimensionality using MiND-MLk\n    (Maximum Likelihood on Minimum Distances, k neighbors).\n    Returns the median of per-point estimates for robustness.\n    Uses FAISS for fast nearest neighbor search.\n    \"\"\"\n    n_samples = data.shape[0]\n    if n_samples &lt; 2:\n        return 0.0\n\n    if precomputed_knn_dist_sq is None:\n        k = min(k, n_samples - 1)\n        if k &lt; 2:\n            return 0.0\n        dist_sq = compute_knn_distances(data, k)\n    else:\n        dist_sq = precomputed_knn_dist_sq\n        k = dist_sq.shape[1]\n        if k &lt; 2:\n            return 0.0\n\n    # Convert to Euclidean distances\n    dist = np.sqrt(dist_sq)\n\n    # Add epsilon to avoid division by zero or log(0)\n    dist = dist + 1e-10\n\n    # The k-th neighbor is at index k-1 (last column)\n    r_k = dist[:, k - 1].reshape(-1, 1)\n    r_j = dist[:, :k - 1]\n\n    log_r_k = np.log(r_k)\n    log_r_j = np.log(r_j)\n\n    sum_log_ratios = np.sum(log_r_k - log_r_j, axis=1)\n\n    with np.errstate(divide='ignore', invalid='ignore'):\n        dim_estimates = (k - 1) / (sum_log_ratios + 1e-10)\n\n    return float(np.median(dim_estimates))\n</code></pre>"},{"location":"api/#effdim.geometry.mle_dimensionality","title":"<code>mle_dimensionality(data, k=10, precomputed_knn_dist_sq=None)</code>","text":"<p>Estimate intrinsic dimensionality using Levina-Bickel MLE. Includes protection against duplicate points (distance=0). Uses FAISS for fast nearest neighbor search.</p> Source code in <code>src/effdim/geometry.py</code> <pre><code>def mle_dimensionality(\n    data: np.ndarray,\n    k: int = 10,\n    precomputed_knn_dist_sq: Optional[np.ndarray] = None\n) -&gt; float:\n    \"\"\"\n    Estimate intrinsic dimensionality using Levina-Bickel MLE.\n    Includes protection against duplicate points (distance=0).\n    Uses FAISS for fast nearest neighbor search.\n    \"\"\"\n    n_samples = data.shape[0]\n    if n_samples &lt; 2:\n        return 0.0\n\n    if precomputed_knn_dist_sq is None:\n        # Cap k to available neighbors\n        k = min(k, n_samples - 1)\n        if k &lt; 2:\n            return 0.0\n\n        # Get squared distances to k neighbors\n        dist_sq = compute_knn_distances(data, k)\n    else:\n        dist_sq = precomputed_knn_dist_sq\n        k = dist_sq.shape[1]\n        if k &lt; 2:\n            return 0.0\n\n    # Convert to Euclidean distances\n    dist = np.sqrt(dist_sq)\n\n    # Add epsilon to avoid division by zero or log(0)\n    dist = dist + 1e-10\n\n    # The k-th neighbor is at index k-1 (last column)\n    r_k = dist[:, k-1]  # shape (n_samples,)\n    r_j = dist[:, :k-1] # shape (n_samples, k-1)\n\n    # Compute sum of log ratios: sum_{j=1}^{k-1} ln(r_k / r_j)\n    # ln(r_k / r_j) = ln(r_k) - ln(r_j)\n\n    # Broadcasting r_k to (n_samples, 1) to subtract\n    log_r_k = np.log(r_k).reshape(-1, 1)\n    log_r_j = np.log(r_j)\n\n    sum_log_ratios = np.sum(log_r_k - log_r_j, axis=1)\n\n    # Estimate for each point: d_i = (k - 1) / sum_log_ratios\n    with np.errstate(divide='ignore', invalid='ignore'):\n        dim_estimates = (k - 1) / (sum_log_ratios + 1e-10)\n\n    # Average over all points to get the estimate\n    return float(np.mean(dim_estimates))\n</code></pre>"},{"location":"api/#effdim.geometry.tle_dimensionality","title":"<code>tle_dimensionality(data, k=10, precomputed_knn_dist_sq=None)</code>","text":"<p>Estimate intrinsic dimensionality using TLE (Tight Localities Estimator). Maximizes likelihood on scale-normalized distances. Uses FAISS for fast nearest neighbor search.</p> Source code in <code>src/effdim/geometry.py</code> <pre><code>def tle_dimensionality(\n    data: np.ndarray,\n    k: int = 10,\n    precomputed_knn_dist_sq: Optional[np.ndarray] = None\n) -&gt; float:\n    \"\"\"\n    Estimate intrinsic dimensionality using TLE\n    (Tight Localities Estimator).\n    Maximizes likelihood on scale-normalized distances.\n    Uses FAISS for fast nearest neighbor search.\n    \"\"\"\n    n_samples = data.shape[0]\n    if n_samples &lt; 2:\n        return 0.0\n\n    if precomputed_knn_dist_sq is None:\n        k = min(k, n_samples - 1)\n        if k &lt; 2:\n            return 0.0\n        dist_sq = compute_knn_distances(data, k)\n    else:\n        dist_sq = precomputed_knn_dist_sq\n        k = dist_sq.shape[1]\n        if k &lt; 2:\n            return 0.0\n\n    # Convert to Euclidean distances\n    dist = np.sqrt(dist_sq)\n\n    # Add epsilon to avoid division by zero or log(0)\n    dist = dist + 1e-10\n\n    r_k = dist[:, k - 1].reshape(-1, 1)\n    r_j = dist[:, :k - 1]\n\n    # Normalized distances\n    u_j = r_j / r_k\n\n    # Per-point estimate: d_i = -(k-1) / \u03a3\u2c7c ln(u_j) = (k-1) / \u03a3\u2c7c ln(1/u_j)\n    log_u = np.log(u_j)\n    neg_sum_log_u = -np.sum(log_u, axis=1)\n\n    with np.errstate(divide='ignore', invalid='ignore'):\n        dim_estimates = (k - 1) / (neg_sum_log_u + 1e-10)\n\n    return float(np.mean(dim_estimates))\n</code></pre>"},{"location":"api/#effdim.geometry.two_nn_dimensionality","title":"<code>two_nn_dimensionality(data, precomputed_knn_dist_sq=None)</code>","text":"<p>Estimate intrinsic dimensionality using Two-NN. Corrects the regression target to -log(1 - F(mu)). Uses FAISS for fast nearest neighbor search.</p> Source code in <code>src/effdim/geometry.py</code> <pre><code>def two_nn_dimensionality(\n    data: np.ndarray,\n    precomputed_knn_dist_sq: Optional[np.ndarray] = None\n) -&gt; float:\n    \"\"\"\n    Estimate intrinsic dimensionality using Two-NN.\n    Corrects the regression target to -log(1 - F(mu)).\n    Uses FAISS for fast nearest neighbor search.\n    \"\"\"\n    n_samples = data.shape[0]\n    if n_samples &lt; 3:\n        return 0.0\n\n    if precomputed_knn_dist_sq is None:\n        # Get squared distances to 2 neighbors (r1, r2)\n        dist_sq = compute_knn_distances(data, 2)\n    else:\n        dist_sq = precomputed_knn_dist_sq\n\n    if dist_sq.shape[1] &lt; 2:\n        return 0.0\n\n    # dist_sq has columns: r1^2, r2^2\n    r1 = np.sqrt(dist_sq[:, 0])\n    r2 = np.sqrt(dist_sq[:, 1])\n\n    # Add epsilon\n    r1 = r1 + 1e-10\n    r2 = r2 + 1e-10\n\n    mu = r2 / r1\n\n    # Sort mu values\n    mu = np.sort(mu)\n\n    # Drop last point to avoid F(mu) = 1 -&gt; log(0)\n    mu = mu[:-1]\n    n_fit = len(mu)\n\n    if n_fit == 0:\n        return 0.0\n\n    # x = ln(mu)\n    x = np.log(mu)\n\n    # y = -ln(1 - F(mu)) = -ln(1 - i/N)\n    # where i is rank 1..n_fit\n    # Note: original paper uses F(mu_i) = i/N where i=1..N\n    # Since we dropped the last one, i goes 1..N-1\n    i = np.arange(1, n_fit + 1)\n    y = -np.log(1.0 - i / n_samples)\n\n    # Linear regression through origin: y = d * x\n    # d = (x . y) / (x . x)\n    x_dot_x = np.dot(x, x)\n    if x_dot_x == 0:\n        return 0.0\n\n    d = np.dot(x, y) / x_dot_x\n\n    return float(d)\n</code></pre>"},{"location":"deployment/","title":"Deployment and Publishing","text":"<p>This guide is for maintainers who publish releases to PyPI.</p>"},{"location":"deployment/#overview","title":"Overview","text":"<p>EffDim uses GitHub Actions to automatically build and publish prebuilt wheels for multiple platforms and Python versions.</p>"},{"location":"deployment/#prerequisites","title":"Prerequisites","text":""},{"location":"deployment/#pypi-account-setup","title":"PyPI Account Setup","text":"<ol> <li>Create a PyPI account at pypi.org</li> <li>Generate an API token:</li> <li>Go to Account Settings \u2192 API Tokens</li> <li>Click \"Add API token\"</li> <li>Name: <code>effdim-github-actions</code></li> <li>Scope: <code>Project: effdim</code></li> <li>Copy the token (starts with <code>pypi-</code>)</li> </ol>"},{"location":"deployment/#github-repository-setup","title":"GitHub Repository Setup","text":"<ol> <li>Go to repository Settings \u2192 Secrets and variables \u2192 Actions</li> <li>Click New repository secret</li> <li>Name: <code>PYPI_API_TOKEN</code></li> <li>Value: [paste PyPI token]</li> <li>Click Add secret</li> </ol>"},{"location":"deployment/#release-process","title":"Release Process","text":""},{"location":"deployment/#1-update-version","title":"1. Update Version","text":"<p>Edit <code>pyproject.toml</code>:</p> <pre><code>[project]\nversion = \"0.1.1\"  # Update this line\n</code></pre>"},{"location":"deployment/#2-update-changelog","title":"2. Update Changelog","text":"<p>Document changes in <code>CHANGELOG.md</code> or release notes:</p> <pre><code>## [0.1.1] - 2024-01-23\n\n### Added\n- New feature X\n- Performance improvements\n\n### Fixed\n- Bug in function Y\n</code></pre>"},{"location":"deployment/#3-commit-changes","title":"3. Commit Changes","text":"<pre><code>git add pyproject.toml CHANGELOG.md\ngit commit -m \"Bump version to 0.1.1\"\ngit push origin main\n</code></pre>"},{"location":"deployment/#4-create-and-push-tag","title":"4. Create and Push Tag","text":"<pre><code># Create annotated tag\ngit tag -a v0.1.1 -m \"Release version 0.1.1\"\n\n# Push tag to trigger workflow\ngit push origin v0.1.1\n</code></pre>"},{"location":"deployment/#5-monitor-build","title":"5. Monitor Build","text":"<ol> <li>Go to Actions tab in GitHub</li> <li>Watch the \"Build and Publish to PyPI\" workflow</li> <li>Verify all jobs complete successfully:</li> <li>\u2705 Build wheels (Linux, macOS, Windows)</li> <li>\u2705 Build source distribution</li> <li>\u2705 Publish to PyPI</li> </ol>"},{"location":"deployment/#6-verify-release","title":"6. Verify Release","text":"<p>After successful workflow:</p> <pre><code># Wait a few minutes for PyPI to update\npip install --upgrade effdim\n\n# Verify version\npython -c \"import effdim; print(effdim.__version__)\"\n</code></pre>"},{"location":"deployment/#build-matrix","title":"Build Matrix","text":"<p>The CI workflow builds wheels for:</p>"},{"location":"deployment/#platforms-and-architectures","title":"Platforms and Architectures","text":""},{"location":"deployment/#linux","title":"Linux","text":"<ul> <li>manylinux: x86_64, aarch64</li> <li>musllinux: x86_64, aarch64</li> </ul>"},{"location":"deployment/#windows","title":"Windows","text":"<ul> <li>x64 (64-bit)</li> <li>x86 (32-bit)</li> </ul>"},{"location":"deployment/#macos","title":"macOS","text":"<ul> <li>x86_64 (Intel) - macOS 13+</li> <li>aarch64 (Apple Silicon) - macOS 14+</li> </ul>"},{"location":"deployment/#python-versions","title":"Python Versions","text":"<p>The workflow uses <code>--find-interpreter</code> to automatically build for all available Python versions (3.8-3.12) on each platform.</p>"},{"location":"deployment/#total-artifacts","title":"Total Artifacts","text":"<p>Approximately 40+ wheels + 1 source distribution per release, covering all combinations of platforms, architectures, and Python versions.</p>"},{"location":"deployment/#workflow-files","title":"Workflow Files","text":""},{"location":"deployment/#githubworkflowsciyml","title":"<code>.github/workflows/CI.yml</code>","text":"<p>The main CI/CD workflow based on maturin's recommended structure:</p> <ul> <li>Triggered by: Pushes to main/master, PRs, tags, or manual dispatch</li> <li>Separate jobs for: linux, musllinux, windows, macos, sdist, release</li> <li>Publishes: To PyPI on version tags (automatically)</li> </ul> <p>Key features:</p> <ul> <li>Uses PyO3/maturin-action</li> <li>Enables sccache for faster builds (disabled on release tags)</li> <li>Builds manylinux and musllinux for maximum compatibility</li> <li>Unique artifact naming prevents conflicts</li> <li>Includes build attestations for security</li> </ul>"},{"location":"deployment/#githubworkflowspublish_docsyml","title":"<code>.github/workflows/publish_docs.yml</code>","text":"<p>Publishes documentation to GitHub Pages (unchanged).</p>"},{"location":"deployment/#testing-before-release","title":"Testing Before Release","text":""},{"location":"deployment/#option-1-manual-workflow-trigger","title":"Option 1: Manual Workflow Trigger","text":"<ol> <li>Go to Actions \u2192 Build and Publish to PyPI</li> <li>Click Run workflow</li> <li>Select branch</li> <li>Review artifacts (won't publish without tag)</li> </ol>"},{"location":"deployment/#option-2-test-with-testpypi","title":"Option 2: Test with TestPyPI","text":"<p>Modify workflow temporarily to use TestPyPI:</p> <pre><code>- name: Publish to TestPyPI\n  uses: PyO3/maturin-action@v1\n  env:\n    MATURIN_PYPI_TOKEN: ${{ secrets.TEST_PYPI_API_TOKEN }}\n  with:\n    command: upload\n    args: --non-interactive --repository-url https://test.pypi.org/legacy/ dist/*\n</code></pre> <p>Then install from TestPyPI:</p> <pre><code>pip install --index-url https://test.pypi.org/simple/ --extra-index-url https://pypi.org/simple effdim\n</code></pre>"},{"location":"deployment/#versioning-strategy","title":"Versioning Strategy","text":"<p>EffDim follows Semantic Versioning:</p> <ul> <li>MAJOR (0.x.x \u2192 1.x.x): Breaking API changes</li> <li>MINOR (x.1.x \u2192 x.2.x): New features, backwards compatible</li> <li>PATCH (x.x.1 \u2192 x.x.2): Bug fixes, backwards compatible</li> </ul>"},{"location":"deployment/#pre-releases","title":"Pre-releases","text":"<p>For beta/RC versions:</p> <pre><code>version = \"0.2.0b1\"  # Beta 1\nversion = \"0.2.0rc1\" # Release Candidate 1\n</code></pre> <p>Tag as: <code>v0.2.0b1</code>, <code>v0.2.0rc1</code></p>"},{"location":"deployment/#troubleshooting","title":"Troubleshooting","text":""},{"location":"deployment/#build-failures","title":"Build Failures","text":"<p>Rust compilation errors:</p> <pre><code># Check locally\nmaturin build --release\n\n# View logs in GitHub Actions\n</code></pre> <p>Python compatibility issues:</p> <ul> <li>Ensure <code>requires-python</code> in <code>pyproject.toml</code> matches tested versions</li> <li>Check minimum Rust version in <code>Cargo.toml</code></li> </ul>"},{"location":"deployment/#upload-failures","title":"Upload Failures","text":"<p>Invalid token:</p> <ul> <li>Regenerate PyPI token</li> <li>Update <code>PYPI_API_TOKEN</code> secret in GitHub</li> </ul> <p>Package name conflict:</p> <ul> <li>First release must be manually created on PyPI</li> <li>Or use different package name</li> </ul> <p>File already exists:</p> <ul> <li>Can't re-upload same version</li> <li>Bump version and retry</li> <li>Use <code>--skip-existing</code> flag (already in workflow)</li> </ul>"},{"location":"deployment/#workflow-not-triggering","title":"Workflow Not Triggering","text":"<p>Tag format issues:</p> <pre><code># Correct\ngit tag v0.1.1\n\n# Incorrect\ngit tag 0.1.1  # Missing 'v' prefix\n</code></pre> <p>Branch protection:</p> <ul> <li>Ensure tags can be pushed to repository</li> <li>Check branch protection rules</li> </ul>"},{"location":"deployment/#rollback-procedure","title":"Rollback Procedure","text":"<p>If a bad release is published:</p>"},{"location":"deployment/#option-1-yank-release-recommended","title":"Option 1: Yank Release (Recommended)","text":"<p>On PyPI:</p> <ol> <li>Go to project page</li> <li>Click on problematic version</li> <li>Click \"Options\" \u2192 \"Yank release\"</li> <li>Publish fixed version</li> </ol>"},{"location":"deployment/#option-2-delete-release","title":"Option 2: Delete Release","text":"<p>\u26a0\ufe0f Not recommended - breaks existing installations</p> <pre><code># Delete tag locally\ngit tag -d v0.1.1\n\n# Delete tag remotely\ngit push origin :refs/tags/v0.1.1\n</code></pre> <p>Then publish corrected version.</p>"},{"location":"deployment/#security","title":"Security","text":""},{"location":"deployment/#api-token-management","title":"API Token Management","text":"<ul> <li>Rotate tokens periodically (every 6-12 months)</li> <li>Use project-scoped tokens (not account-wide)</li> <li>Never commit tokens to repository</li> <li>Store in GitHub Secrets only</li> </ul>"},{"location":"deployment/#dependency-security","title":"Dependency Security","text":"<p>Automated security scanning:</p> <ul> <li>Dependabot alerts (GitHub)</li> <li><code>cargo audit</code> for Rust dependencies</li> <li><code>pip-audit</code> for Python dependencies</li> </ul>"},{"location":"deployment/#maintenance-checklist","title":"Maintenance Checklist","text":"<p>Before each release:</p> <ul> <li>[ ] All tests passing</li> <li>[ ] Documentation updated</li> <li>[ ] Changelog updated</li> <li>[ ] Version bumped</li> <li>[ ] Dependencies updated</li> <li>[ ] Security audit clean</li> <li>[ ] Performance benchmarks run</li> <li>[ ] Breaking changes documented</li> </ul>"},{"location":"deployment/#additional-resources","title":"Additional Resources","text":"<ul> <li>Maturin Documentation</li> <li>PyPI Help</li> <li>GitHub Actions Documentation</li> <li>Semantic Versioning</li> </ul>"},{"location":"theory/","title":"Theory &amp; Estimators","text":"<p>EffDim implements a variety of estimators for \"effective dimensionality\" (ED). These can be broadly categorized into Spectral Estimators, which operate on the eigenvalues (spectrum) of the data's covariance/correlation matrix, and Geometric Estimators, which operate on the distances between data points.</p>"},{"location":"theory/#mathematical-foundation","title":"Mathematical Foundation","text":""},{"location":"theory/#eigenvalue-computation","title":"Eigenvalue Computation","text":"<p>For a data matrix \\(\\mathbf{X} \\in \\mathbb{R}^{n \\times p}\\) with \\(n\\) samples and \\(p\\) features:</p> <ol> <li> <p>Centering: The data is centered by subtracting the mean: \\(\\tilde{\\mathbf{X}} = \\mathbf{X} - \\frac{1}{n}\\mathbf{1}\\mathbf{1}^T\\mathbf{X}\\)</p> </li> <li> <p>SVD Decomposition: \\(\\tilde{\\mathbf{X}} = \\mathbf{U}\\mathbf{S}\\mathbf{V}^T\\) where \\(\\mathbf{S}\\) contains singular values \\(s_1 \\ge s_2 \\ge \\dots \\ge s_{\\min(n,p)} \\ge 0\\)</p> </li> <li> <p>Eigenvalue Calculation: The eigenvalues of the sample covariance matrix are:    $$ \\lambda_i = \\frac{s_i^2}{n-1} $$</p> </li> </ol> <p>This follows from \\(\\mathbf{C} = \\frac{1}{n-1}\\tilde{\\mathbf{X}}^T\\tilde{\\mathbf{X}} = \\frac{1}{n-1}\\mathbf{V}\\mathbf{S}^2\\mathbf{V}^T\\)</p> <ol> <li>Normalization: The normalized spectrum (probability distribution) is:    $$ p_i = \\frac{\\lambda_i}{\\sum_{j=1}^{D} \\lambda_j} $$</li> </ol> <p>where \\(D = \\min(n-1, p)\\) is the maximum number of non-zero eigenvalues.</p> <p>Note: The implementation uses \\((n-1)\\) normalization (sample covariance, unbiased estimator) rather than \\(n\\) (population covariance).</p>"},{"location":"theory/#spectral-estimators","title":"Spectral Estimators","text":"<p>These methods rely on the spectrum \\(\\lambda_1 \\ge \\lambda_2 \\ge \\dots \\ge \\lambda_D \\ge 0\\) of the covariance matrix. We define the normalized spectrum as \\(p_i = \\frac{\\lambda_i}{\\sum_j \\lambda_j}\\), which can be treated as a probability distribution.</p>"},{"location":"theory/#pca-explained-variance","title":"PCA Explained Variance","text":"<p>The classic approach used in Principal Component Analysis. It defines the effective dimension as the number of components required to explain a certain fraction (threshold) of the total variance.</p> <p>Formula: $$ ED_{PCA}(\\tau) = \\min \\left{ k \\mid \\frac{\\sum_{i=1}^k \\lambda_i}{\\sum_{j=1}^D \\lambda_j} \\ge \\tau \\right} $$</p> <p>where \\(\\tau\\) is the threshold (default 0.95).</p> <p>Implementation Details: - Uses <code>np.searchsorted</code> for efficient binary search - Returns integer number of components - Common thresholds: 0.90, 0.95, 0.99</p> <p>Interpretation: - Direct measure of intrinsic dimensionality - Easy to interpret: \"95% of variance explained by \\(k\\) dimensions\" - Depends on threshold choice</p>"},{"location":"theory/#participation-ratio-pr","title":"Participation Ratio (PR)","text":"<p>Widely used in physics and neuroscience to quantify the \"spread\" of the spectrum. If the variance is equally distributed across \\(N\\) dimensions, \\(PR=N\\). If it is concentrated in 1 dimension, \\(PR=1\\).</p> <p>Formula: $$ PR = \\frac{\\left(\\sum_{i=1}^D \\lambda_i\\right)^2}{\\sum_{i=1}^D \\lambda_i^2} = \\frac{1}{\\sum_{i=1}^D p_i^2} $$</p> <p>Mathematical Equivalence: The Participation Ratio is equivalent to the R\u00e9nyi Effective Dimension with \\(\\alpha = 2\\): $$ PR = \\exp\\left(\\frac{1}{1-2} \\ln \\sum_i p_i^2\\right) = \\left(\\sum_i p_i^2\\right)^{-1} = \\frac{1}{\\sum_i p_i^2} $$</p> <p>Implementation Details: - Works with eigenvalues \\(\\lambda_i\\) directly (scale-invariant) - Protects against division by zero - Continuous (non-integer) output</p> <p>Interpretation: - \\(PR = D\\) when all eigenvalues are equal (maximum spread) - \\(PR = 1\\) when one eigenvalue dominates (minimum spread) - Robust to small eigenvalues (inverse quadratic weighting)</p>"},{"location":"theory/#shannon-effective-dimension","title":"Shannon Effective Dimension","text":"<p>Based on the Shannon entropy of the spectral distribution. It corresponds to the exponential of the entropy.</p> <p>Formula: $$ H = - \\sum_{i=1}^D p_i \\ln p_i $$ $$ ED_{Shannon} = \\exp(H) $$</p> <p>Implementation Details: - Filters zero probabilities before computing \\(\\ln p_i\\) to avoid \\(\\ln(0)\\) - Returns exponential of entropy (not entropy itself) - Also known as \"Effective Rank\" or \"Perplexity\"</p> <p>Interpretation: - \\(ED_{Shannon} = D\\) when all eigenvalues are equal - \\(ED_{Shannon} = 1\\) when one eigenvalue dominates - Represents the \"number of equally likely states\" - More sensitive to tail of distribution than PR - Related to information content of the spectrum</p> <p>Connection to Information Theory: The Shannon ED can be interpreted as the effective number of \"independent\" dimensions, where each dimension contributes equally to the total variance in an information-theoretic sense.</p>"},{"location":"theory/#renyi-effective-dimension-alpha-entropy","title":"R\u00e9nyi Effective Dimension (Alpha-Entropy)","text":"<p>A generalization of the Shannon dimension using R\u00e9nyi entropy of order \\(\\alpha\\).</p> <p>Formula: $$ H_\\alpha = \\frac{1}{1-\\alpha} \\ln \\left(\\sum_{i=1}^D p_i^\\alpha\\right) $$ $$ ED_{\\alpha} = \\exp(H_\\alpha) = \\left(\\sum_{i=1}^D p_i^\\alpha\\right)^{\\frac{1}{1-\\alpha}} $$</p> <p>Special Cases: *   \\(\\alpha \\to 1\\): Converges to Shannon Effective Dimension (via L'H\u00f4pital's rule) *   \\(\\alpha = 2\\): \\(ED_2 = \\left(\\sum_i p_i^2\\right)^{-1} = PR\\) (Participation Ratio) *   \\(\\alpha = 0\\): Counts non-zero eigenvalues (algebraic rank) *   \\(\\alpha \\to \\infty\\): \\(ED_\\infty = 1/\\max_i p_i\\) (inverse of largest probability)</p> <p>Implementation Details: - Validates \\(\\alpha &gt; 0\\) and \\(\\alpha \\neq 1\\) - For \\(\\alpha = 1\\), use Shannon ED instead - Protects against zero denominator</p> <p>Interpretation: - \\(\\alpha\\) controls sensitivity to rare vs common components - Small \\(\\alpha\\) (e.g., 0.5): More weight to rare eigenvalues - Large \\(\\alpha\\) (e.g., 5): More weight to dominant eigenvalues - Provides a one-parameter family interpolating between different notions of dimension</p> <p>When to Use Which Alpha: - \\(\\alpha = 2\\) (PR): Balanced measure, equivalent to inverse Simpson index - \\(\\alpha = 3, 4, 5\\): Emphasize dominant components - For general analysis, \\(\\alpha = 2\\) (PR) is most common</p>"},{"location":"theory/#effective-rank","title":"Effective Rank","text":"<p>Often used in matrix completion and low-rank approximation contexts. EffDim implements this as an alias for Shannon Effective Dimension.</p>"},{"location":"theory/#geometric-mean-dimension","title":"Geometric Mean Dimension","text":"<p>A dimension proxy based on the ratio of the arithmetic mean to the geometric mean of the spectrum.</p> <p>Formula: $$ d_{GM} = \\frac{\\text{AM}(\\lambda)}{\\text{GM}(\\lambda)} = \\frac{\\frac{1}{D'} \\sum_{i=1}^{D'} \\lambda_i}{\\left(\\prod_{i=1}^{D'} \\lambda_i\\right)^{1/D'}} $$</p> <p>where \\(D'\\) is the number of positive eigenvalues, \\(\\lambda_i &gt; 0\\).</p> <p>Equivalent Form: $$ d_{GM} = \\frac{\\frac{1}{D'} \\sum_{i=1}^{D'} \\lambda_i}{\\exp\\left(\\frac{1}{D'} \\sum_{i=1}^{D'} \\ln \\lambda_i\\right)} $$</p> <p>Implementation Details: - Filters zero/negative eigenvalues before computation - Uses \\(\\ln\\) for numerical stability in geometric mean - Scale-invariant (gives same result for eigenvalues or probabilities) - Returns 0.0 if no positive eigenvalues</p> <p>Interpretation: - \\(d_{GM} = 1\\) when all positive eigenvalues are equal - \\(d_{GM} &gt; 1\\) indicates spread in the spectrum - By AM-GM inequality: \\(d_{GM} \\ge 1\\) always - Larger values indicate more inequality in eigenvalue distribution - Related to condition number of the covariance matrix</p> <p>Note on Scale Invariance: Because \\(\\text{AM}(c\\lambda) = c \\cdot \\text{AM}(\\lambda)\\) and \\(\\text{GM}(c\\lambda) = c \\cdot \\text{GM}(\\lambda)\\), the ratio is unchanged by scaling. Therefore, using eigenvalues \\(\\lambda_i\\) or normalized probabilities \\(p_i\\) yields the same result.</p>"},{"location":"theory/#stable-rank","title":"Stable Rank","text":"<p>A stable alternative to the algebraic rank, often used in high-dimensional probability. It is robust to small perturbations of the singular values.</p> \\[ R_{stable} = \\frac{\\sum_i \\lambda_i}{\\max_i \\lambda_i} \\] <p>where \\(\\lambda_i\\) are the eigenvalues (variances).</p>"},{"location":"theory/#numerical-rank-epsilon-rank","title":"Numerical Rank (Epsilon-Rank)","text":"<p>The number of singular values greater than a specific threshold \\(\\epsilon\\).</p> \\[ rank_\\epsilon(A) = | \\{ \\sigma_i \\mid \\sigma_i &gt; \\epsilon \\} | \\] <p>If \\(\\epsilon\\) is not provided, it defaults to a value based on the machine precision and the largest singular value.</p>"},{"location":"theory/#cumulative-eigenvalue-ratio-cer","title":"Cumulative Eigenvalue Ratio (CER)","text":"<p>A weighted sum of the normalized spectrum, giving more weight to earlier components.</p> \\[ CER = \\sum_{i=1}^D w_i p_i \\] <p>where weights decrease linearly from 1 to 0.</p>"},{"location":"theory/#geometric-estimators","title":"Geometric Estimators","text":"<p>These methods estimate the intrinsic dimension (ID) of the data manifold based on local neighborhoods, without relying on global projections like PCA.</p>"},{"location":"theory/#knn-intrinsic-dimension-mle","title":"kNN Intrinsic Dimension (MLE)","text":"<p>The Maximum Likelihood Estimator proposed by Levina and Bickel (2005). It estimates dimension by examining the ratio of distances to the \\(k\\)-th nearest neighbor.</p> <p>Reference: Levina, E., &amp; Bickel, P. J. (2005). Maximum likelihood estimation of intrinsic dimension. Advances in Neural Information Processing Systems 17 (NIPS 2004).</p> <p>Formula: For each point \\(x_i\\), let \\(r_1(x_i) \\le r_2(x_i) \\le \\dots \\le r_k(x_i)\\) be the distances to its \\(k\\) nearest neighbors. The local dimension estimate is:</p> \\[ \\hat{d}_k(x_i) = \\left[ \\frac{1}{k-1} \\sum_{j=1}^{k-1} \\ln \\frac{r_k(x_i)}{r_j(x_i)} \\right]^{-1} = \\frac{k-1}{\\sum_{j=1}^{k-1} \\ln r_k(x_i) - \\ln r_j(x_i)} \\] <p>The global estimate is the average over all points: $$ \\hat{d}k = \\frac{1}{n} \\sum_k(x_i) $$}^n \\hat{d</p> <p>Implementation Details: - Uses FAISS library for efficient kNN search with L2 distance - Adds \\(\\epsilon = 10^{-10}\\) to distances to prevent \\(\\ln(0)\\) - Uses <code>np.errstate</code> to handle potential numerical issues - Default \\(k = 10\\) neighbors - Complexity: \\(O(n^2)\\) naive, \\(O(n \\log n)\\) with FAISS indexing</p> <p>Numerical Stability: - \\(\\ln(r_k/r_j) = \\ln r_k - \\ln r_j\\) computed in log-space for stability - Epsilon added to denominator to prevent division by zero - Handles duplicate points (distance = 0) gracefully</p> <p>When to Use: - Data lies on or near a low-dimensional manifold - Sufficient sample density (at least \\(k\\) neighbors per point) - Metric (Euclidean) distance is meaningful</p> <p>Choosing k: - Too small \\(k\\): High variance, sensitive to noise - Too large \\(k\\): Bias if curvature is significant - Typical range: \\(k \\in [5, 20]\\) - Rule of thumb: \\(k \\approx \\sqrt{n}\\) for small datasets</p>"},{"location":"theory/#two-nn","title":"Two-NN","text":"<p>A robust estimator proposed by Facco et al. (2017) that relies only on the distances to the first two nearest neighbors. It is less sensitive to density variations and curvature than standard kNN.</p> <p>Reference: Facco, E., d'Errico, M., Rodriguez, A., &amp; Laio, A. (2017). Estimating the intrinsic dimension of datasets by a minimal neighborhood information. Scientific Reports, 7(1), 12140. DOI: 10.1038/s41598-017-11873-y</p> <p>Theory: Assumes data is uniformly distributed on a \\(d\\)-dimensional manifold. The ratio \\(\\mu_i = r_2(x_i) / r_1(x_i)\\) follows a distribution with CDF: $$ F(\\mu) = 1 - \\mu^{-d} $$</p> <p>Taking logarithms: $$ -\\ln(1 - F(\\mu)) = d \\cdot \\ln(\\mu) $$</p> <p>Formula: 1. For each point \\(x_i\\), compute \\(\\mu_i = r_2(x_i) / r_1(x_i)\\) 2. Sort the \\(\\mu_i\\) values: \\(\\mu_{(1)} \\le \\mu_{(2)} \\le \\dots \\le \\mu_{(n)}\\) 3. Use empirical CDF: \\(F(\\mu_{(i)}) = i/n\\) for \\(i = 1, \\dots, n-1\\) (drop last to avoid \\(\\ln(0)\\)) 4. Fit linear regression through origin:    $$ y_i = d \\cdot x_i $$    where \\(x_i = \\ln(\\mu_{(i)})\\) and \\(y_i = -\\ln(1 - i/n)\\) 5. Estimate: \\(\\hat{d} = \\frac{\\sum_i x_i y_i}{\\sum_i x_i^2}\\)</p> <p>Implementation Details: - Uses FAISS for kNN search (k=2) - Adds \\(\\epsilon = 10^{-10}\\) to distances - Drops last point to avoid \\(F=1\\) causing \\(\\ln(0)\\) - Linear regression uses dot product formula (no intercept) - Complexity: \\(O(n \\log n)\\) with FAISS</p> <p>Numerical Stability: - Epsilon prevents division by zero in \\(\\mu = r_2/r_1\\) - Dropping last point prevents \\(\\ln(1-1) = \\ln(0)\\) - Checks for zero variance in \\(\\mu\\) values</p> <p>Advantages over MLE: - More robust to density variations - Less sensitive to manifold curvature - Requires only 2 neighbors (works with smaller samples) - No parameter tuning required</p> <p>When to Use: - Preferred for datasets with non-uniform density - When sample size is limited - When you want parameter-free estimation</p>"},{"location":"theory/#danco-dimensionality-from-angle-and-norm-concentration","title":"DANCo (Dimensionality from Angle and Norm Concentration)","text":"<p>Exploits the concentration of angles between nearest neighbor vectors to estimate intrinsic dimension.</p> <p>Reference: Ceruti, C., et al. (2012). DANCo: Dimensionality from Angle and Norm Concentration. arXiv:1206.3881.</p> <p>Theory: In a \\(d\\)-dimensional space, the angles between random vectors concentrate around \\(\\pi/2\\) as \\(d\\) increases. Specifically, for two random unit vectors in \\(\\mathbb{R}^d\\), the expected value of \\(\\cos^2(\\theta) \\approx 1/d\\).</p> <p>Formula: 1. For each point \\(x_i\\), compute vectors \\(v_{ij} = x_j - x_i\\) to its \\(k\\) nearest neighbors. 2. Normalize to unit vectors: \\(\\hat{v}_{ij} = v_{ij} / \\|v_{ij}\\|\\). 3. Compute pairwise cosines: \\(\\cos(\\theta_{jl}) = \\hat{v}_{ij} \\cdot \\hat{v}_{il}\\). 4. Estimate: \\(\\hat{d} = 1 / \\overline{\\cos^2(\\theta)}\\) where the average is over all points and all pairs.</p> <p>Implementation Details: - Uses FAISS for efficient kNN search - Adds \\(\\epsilon = 10^{-10}\\) to norms to prevent division by zero - Default \\(k = 10\\) neighbors - Uses <code>np.einsum</code> for efficient pairwise cosine computation</p> <p>When to Use: - When angle-based estimation is preferred over distance-based - For data with complex geometric structure</p>"},{"location":"theory/#mind-mli-minimum-distance-single-neighbor","title":"MiND-MLi (Minimum Distance \u2014 Single Neighbor)","text":"<p>Maximum Likelihood Estimator based on the distribution of nearest neighbor distances only.</p> <p>Reference: Rozza, A., et al. (2012). Novel high intrinsic dimensionality estimators. Machine Learning, 89(1-2), 37-65.</p> <p>Formula: 1. For each point \\(x_i\\), compute the nearest neighbor distance \\(r_1(x_i)\\). 2. Let \\(r_{\\max} = \\max_i r_1(x_i)\\). 3. Estimate: \\(\\hat{d} = n / \\sum_{i=1}^n \\ln(r_{\\max} / r_1(x_i))\\).</p> <p>Implementation Details: - Uses only the single nearest neighbor distance - Returns 0.0 if all distances are equal (degenerate case) - Requires at least 3 points</p> <p>When to Use: - When minimal neighborhood information is available - For quick estimates with low computational cost</p>"},{"location":"theory/#mind-mlk-minimum-distance-k-neighbors","title":"MiND-MLk (Minimum Distance \u2014 k Neighbors)","text":"<p>Extension of the Levina-Bickel MLE using the median of per-point estimates for robustness against outliers.</p> <p>Reference: Rozza, A., et al. (2012). Novel high intrinsic dimensionality estimators. Machine Learning, 89(1-2), 37-65.</p> <p>Formula: Same per-point estimates as kNN MLE: $$ \\hat{d}k(x_i) = \\frac{k-1}{\\sum $$}^{k-1} \\ln r_k(x_i) - \\ln r_j(x_i)</p> <p>Global estimate uses the median instead of the mean: $$ \\hat{d}_k = \\text{median}\\left(\\hat{d}_k(x_1), \\dots, \\hat{d}_k(x_n)\\right) $$</p> <p>Advantages over standard MLE: - More robust to outliers and density inhomogeneities - The median is less sensitive to extreme per-point estimates</p>"},{"location":"theory/#ess-expected-simplex-skewness","title":"ESS (Expected Simplex Skewness)","text":"<p>Estimates dimension by analyzing the skewness of local simplices formed by nearest neighbors.</p> <p>Reference: Johnsson, K., et al. (2015). Low bias local intrinsic dimension estimation from expected simplex skewness. IEEE Trans. PAMI, 37(1), 196-202.</p> <p>Theory: For each point, the \\(k\\) nearest neighbors form a simplex. The \"skewness\" is measured as the squared norm of the mean of unit direction vectors. In \\(d\\) dimensions, the expected squared norm of the mean of \\(k\\) random unit vectors is approximately \\(1/(k \\cdot d)\\).</p> <p>Formula: 1. For each point \\(x_i\\), compute unit vectors \\(\\hat{v}_{ij}\\) to its \\(k\\) neighbors. 2. Compute centroid: \\(\\bar{v}_i = \\frac{1}{k}\\sum_{j=1}^k \\hat{v}_{ij}\\). 3. Compute skewness: \\(S_i = \\|\\bar{v}_i\\|^2\\). 4. Average: \\(\\bar{S} = \\frac{1}{n}\\sum_{i=1}^n S_i\\). 5. Estimate: \\(\\hat{d} = 1 / (k \\cdot \\bar{S})\\).</p> <p>When to Use: - When low-bias estimation is important - For manifolds with moderate curvature</p>"},{"location":"theory/#tle-tight-localities-estimator","title":"TLE (Tight Localities Estimator)","text":"<p>Maximizes likelihood on scale-normalized distances, making it more robust to density variations.</p> <p>Reference: Amsaleg, L., et al. (2019). Intrinsic dimensionality estimation within tight localities. SDM 2019.</p> <p>Theory: For each point, the distances to \\(k\\) neighbors are normalized by the \\(k\\)-th neighbor distance: \\(u_j = r_j / r_k\\). Under a \\(d\\)-dimensional uniform distribution, each \\(u_j\\) follows a \\(\\text{Beta}(d, 1)\\) distribution with PDF \\(p(u) = d \\cdot u^{d-1}\\).</p> <p>Formula: $$ \\hat{d}i = \\frac{-(k-1)}{\\sum $$}^{k-1} \\ln u_j} = \\frac{k-1}{\\sum_{j=1}^{k-1} \\ln(r_k / r_j)</p> <p>The global estimate is \\(\\hat{d} = \\frac{1}{n}\\sum_{i=1}^n \\hat{d}_i\\).</p> <p>Implementation Details: - Mathematically equivalent to the Levina-Bickel MLE - The per-point normalization by \\(r_k\\) provides scale invariance</p>"},{"location":"theory/#gmst-geodesic-minimum-spanning-tree","title":"GMST (Geodesic Minimum Spanning Tree)","text":"<p>Estimates dimension from how the total length of the Minimum Spanning Tree (MST) scales with the number of points.</p> <p>Reference: Costa, J. A., &amp; Hero, A. O. (2004). Geodesic entropic graphs for dimension and entropy estimation in manifold learning. IEEE Trans. Signal Processing, 52(8), 2210-2221.</p> <p>Theory: For \\(n\\) points sampled uniformly from a \\(d\\)-dimensional manifold (\\(d &gt; 1\\)), the total MST edge weight scales as: $$ L_{\\text{MST}} \\propto n^{(d-1)/d} $$</p> <p>Taking logarithms: \\(\\ln L_{\\text{MST}} = \\alpha \\cdot \\ln n + c\\), where \\(\\alpha = (d-1)/d\\), giving \\(d = 1/(1-\\alpha)\\).</p> <p>Formula: 1. Take subsamples of sizes \\(n_1, n_2, \\dots\\) from the data. 2. For each subsample, compute the MST and its total edge weight \\(L_i\\). 3. Fit linear regression: \\(\\ln L_i = \\alpha \\cdot \\ln n_i + c\\). 4. Estimate: \\(\\hat{d} = 1 / (1 - \\alpha)\\).</p> <p>Geodesic Mode: When <code>geodesic=True</code>, distances are computed along the data manifold using shortest paths on a \\(k\\)-NN graph, rather than straight-line Euclidean distances.</p> <p>Implementation Details: - Uses <code>scipy.sparse.csgraph.minimum_spanning_tree</code> for MST computation - Uses <code>scipy.spatial.distance.pdist</code> for Euclidean distances - Uses <code>sklearn.neighbors.kneighbors_graph</code> + <code>scipy.sparse.csgraph.shortest_path</code> for geodesic distances - Subsamples at sizes \\([n/8, n/4, n/2, n]\\) with a fixed random seed for reproducibility - Requires at least 10 points</p> <p>When to Use: - When data lies on a curved manifold (use geodesic mode) - When graph-based analysis is preferred - For datasets where local methods may be biased</p>"},{"location":"theory/#assumptions-and-limitations","title":"Assumptions and Limitations","text":""},{"location":"theory/#general-assumptions","title":"General Assumptions","text":"<p>Data Requirements: - Data should be centered (automatically handled in implementation) - Sufficient sample size: \\(n \\gg d\\) for reliable spectral estimates - Sufficient sample size: \\(n \\ge 2k\\) for geometric estimates - No missing values (NaN/Inf not handled)</p> <p>Spectral Estimators: - Assume linear (global) structure - Best for: Gaussian or approximately Gaussian data - May overestimate dimension for highly curved manifolds - Sensitive to outliers (affect eigenvalue spectrum) - Assume data lies in Euclidean space (not on a non-linear manifold)</p> <p>Geometric Estimators: - Assume local manifold structure - Best for: Data on or near low-dimensional manifolds - Require sufficient local sample density - Assume metric (Euclidean) distance is meaningful - May fail for highly sparse or non-uniformly sampled data</p>"},{"location":"theory/#known-limitations","title":"Known Limitations","text":"<p>Computational Complexity: - SVD: \\(O(\\min(n^2p, np^2))\\) for full SVD, \\(O(npk)\\) for randomized SVD - MLE: \\(O(n^2)\\) for exact kNN, \\(O(n \\log n)\\) with FAISS indexing - Two-NN: \\(O(n \\log n)\\) with FAISS</p> <p>Sample Size Dependencies: - Spectral methods: Eigenvalues stabilize when \\(n \\ge 5d\\) (rule of thumb) - MLE: Requires at least \\(k+1\\) points, converges when \\(n \\ge 100\\) - Two-NN: Requires at least 3 points, more robust for small \\(n\\) than MLE - Small \\(n\\): High variance in all estimates</p> <p>High-Dimensional Data (\\(p \\gg n\\)): - Only \\(\\min(n-1, p)\\) eigenvalues are non-zero - Covariance matrix is rank-deficient - Spectral methods still work but are limited by \\(n\\) - Consider using randomized SVD for efficiency</p> <p>Numerical Stability: - Very small eigenvalues (\\(\\lambda &lt; 10^{-10}\\)) may be unstable - Very large condition numbers may cause issues - Log of very small values protected by epsilon addition - Division by zero protected throughout</p>"},{"location":"theory/#when-to-use-which-estimator","title":"When to Use Which Estimator","text":"<p>Use PCA Explained Variance when: - You need an interpretable, threshold-based dimension - Data is approximately Gaussian - You want to choose how much information to retain</p> <p>Use Participation Ratio when: - You want a continuous measure - You need mathematical equivalence to R\u00e9nyi-2 - Comparing across different datasets</p> <p>Use Shannon ED when: - You want an information-theoretic measure - You need sensitivity to the full spectrum (including tail) - Comparing to entropy-based methods</p> <p>Use MLE when: - Data lies on a non-linear manifold - You have sufficient sample density - You can tune the \\(k\\) parameter</p> <p>Use Two-NN when: - You want a robust, parameter-free geometric method - Sample size is limited - Data has non-uniform density</p>"},{"location":"theory/#references","title":"References","text":"<p>Spectral Methods: - Roy, O., &amp; Vetterli, M. (2007). The effective rank: A measure of effective dimensionality. EUSIPCO.</p> <p>Geometric Methods: - Levina, E., &amp; Bickel, P. J. (2005). Maximum likelihood estimation of intrinsic dimension. NIPS 2004. - Facco, E., d'Errico, M., Rodriguez, A., &amp; Laio, A. (2017). Estimating the intrinsic dimension of datasets by a minimal neighborhood information. Scientific Reports, 7(1), 12140. DOI: 10.1038/s41598-017-11873-y - Ceruti, C., et al. (2012). DANCo: Dimensionality from Angle and Norm Concentration. arXiv:1206.3881. - Rozza, A., et al. (2012). Novel high intrinsic dimensionality estimators. Machine Learning, 89(1-2), 37-65. - Johnsson, K., et al. (2015). Low bias local intrinsic dimension estimation from expected simplex skewness. IEEE Trans. PAMI, 37(1), 196-202. - Amsaleg, L., et al. (2019). Intrinsic dimensionality estimation within tight localities. SDM 2019. - Costa, J. A., &amp; Hero, A. O. (2004). Geodesic entropic graphs for dimension and entropy estimation in manifold learning. IEEE Trans. Signal Processing, 52(8), 2210-2221.</p> <p>Survey: - Camastra, F., &amp; Staiano, A. (2016). Intrinsic dimension estimation: Advances and open problems. Information Sciences, 328, 26-41.</p>"},{"location":"tutorials/advanced_geometric_analysis/","title":"Advanced Geometric Analysis","text":"<p>This tutorial showcases the advanced geometric estimators available in EffDim: DANCo, MiND-MLi, MiND-MLk, ESS, TLE, and GMST.</p>"},{"location":"tutorials/advanced_geometric_analysis/#overview-of-new-estimators","title":"Overview of New Estimators","text":"Estimator Key Idea Parameters DANCo Angle concentration between neighbor vectors <code>k</code> (neighbors) MiND-MLi Nearest-neighbor distance distribution None MiND-MLk k-neighbor distances, median aggregation <code>k</code> (neighbors) ESS Simplex skewness of local neighborhoods <code>k</code> (neighbors) TLE Scale-normalized distance likelihood <code>k</code> (neighbors) GMST MST length scaling with sample size <code>geodesic</code> (bool)"},{"location":"tutorials/advanced_geometric_analysis/#example-swiss-roll-analysis","title":"Example: Swiss Roll Analysis","text":"<p>The Swiss Roll is a classic benchmark \u2014 a 2D manifold embedded in 3D space.</p> <pre><code>import numpy as np\nimport effdim\nfrom sklearn.datasets import make_swiss_roll\n\n# Generate Swiss Roll (intrinsic dimension = 2)\nX, _ = make_swiss_roll(n_samples=2000, noise=0.01, random_state=42)\n\nresults = effdim.compute_dim(X)\n\nprint(\"=== Spectral Estimators ===\")\nprint(f\"PCA (95% variance):  {results['pca_explained_variance_95']}\")\nprint(f\"Participation Ratio: {results['participation_ratio']:.2f}\")\n\nprint(\"\\n=== Classic Geometric Estimators ===\")\nprint(f\"kNN MLE:   {results['mle_dimensionality']:.2f}\")\nprint(f\"Two-NN:    {results['two_nn_dimensionality']:.2f}\")\n\nprint(\"\\n=== New Geometric Estimators ===\")\nprint(f\"DANCo:     {results['danco_dimensionality']:.2f}\")\nprint(f\"MiND-MLi:  {results['mind_mli_dimensionality']:.2f}\")\nprint(f\"MiND-MLk:  {results['mind_mlk_dimensionality']:.2f}\")\nprint(f\"ESS:       {results['ess_dimensionality']:.2f}\")\nprint(f\"TLE:       {results['tle_dimensionality']:.2f}\")\nprint(f\"GMST:      {results['gmst_dimensionality']:.2f}\")\n</code></pre>"},{"location":"tutorials/advanced_geometric_analysis/#example-comparing-estimators-on-known-manifolds","title":"Example: Comparing Estimators on Known Manifolds","text":"<pre><code>import numpy as np\nimport effdim\n\n# 1D curve in 3D (helix)\nt = np.linspace(0, 4 * np.pi, 500)\nhelix = np.column_stack([np.cos(t), np.sin(t), t / (4 * np.pi)])\nresults_1d = effdim.compute_dim(helix)\n\n# 2D plane in 5D\nnp.random.seed(42)\nplane_2d = np.random.randn(500, 2)\nembedding = np.random.randn(2, 5)\ndata_5d = plane_2d @ embedding\nresults_2d = effdim.compute_dim(data_5d)\n\n# 5D Gaussian\ndata_5d_full = np.random.randn(500, 5)\nresults_5d = effdim.compute_dim(data_5d_full)\n\nprint(\"Estimator         | 1D Helix | 2D in 5D | 5D Gaussian\")\nprint(\"-\" * 55)\nfor key in ['mle_dimensionality', 'two_nn_dimensionality',\n            'mind_mlk_dimensionality', 'tle_dimensionality']:\n    print(f\"{key:25s} | {results_1d[key]:7.2f}  | {results_2d[key]:7.2f}  | {results_5d[key]:7.2f}\")\n</code></pre>"},{"location":"tutorials/advanced_geometric_analysis/#using-gmst-with-geodesic-distances","title":"Using GMST with Geodesic Distances","text":"<p>The GMST estimator supports geodesic distances, which follow the manifold surface rather than cutting through ambient space. This is especially useful for curved manifolds.</p> <pre><code>import numpy as np\nfrom effdim.geometry import gmst_dimensionality\nfrom sklearn.datasets import make_swiss_roll\n\nX, _ = make_swiss_roll(n_samples=500, noise=0.01, random_state=42)\n\n# Euclidean mode\ndim_euclidean = gmst_dimensionality(X, geodesic=False)\nprint(f\"GMST (Euclidean): {dim_euclidean:.2f}\")\n\n# Geodesic mode\ndim_geodesic = gmst_dimensionality(X, geodesic=True)\nprint(f\"GMST (Geodesic):  {dim_geodesic:.2f}\")\n</code></pre>"},{"location":"tutorials/advanced_geometric_analysis/#using-individual-estimators","title":"Using Individual Estimators","text":"<p>Each estimator can be called directly for fine-grained control:</p> <pre><code>import numpy as np\nfrom effdim.geometry import (\n    danco_dimensionality,\n    mind_mli_dimensionality,\n    mind_mlk_dimensionality,\n    ess_dimensionality,\n    tle_dimensionality,\n    gmst_dimensionality,\n)\n\nnp.random.seed(42)\ndata = np.random.randn(200, 5)\n\n# DANCo with custom k\nprint(f\"DANCo (k=5):  {danco_dimensionality(data, k=5):.2f}\")\nprint(f\"DANCo (k=15): {danco_dimensionality(data, k=15):.2f}\")\n\n# MiND variants\nprint(f\"MiND-MLi:     {mind_mli_dimensionality(data):.2f}\")\nprint(f\"MiND-MLk:     {mind_mlk_dimensionality(data, k=10):.2f}\")\n\n# ESS and TLE\nprint(f\"ESS (k=10):   {ess_dimensionality(data, k=10):.2f}\")\nprint(f\"TLE (k=10):   {tle_dimensionality(data, k=10):.2f}\")\n\n# GMST\nprint(f\"GMST:         {gmst_dimensionality(data):.2f}\")\n</code></pre>"},{"location":"tutorials/advanced_geometric_analysis/#when-to-use-which-estimator","title":"When to Use Which Estimator","text":"<ul> <li>DANCo: Best when angle-based geometric analysis is informative; works well for higher-dimensional data.</li> <li>MiND-MLi: Quick, parameter-free estimate using only nearest-neighbor distances. Best for quick sanity checks.</li> <li>MiND-MLk: More robust version of MLE using median aggregation. Use when outlier robustness is important.</li> <li>ESS: Low-bias estimator based on simplex geometry. Good for manifolds with moderate curvature.</li> <li>TLE: Scale-invariant due to per-point normalization. Use when data has non-uniform density.</li> <li>GMST: Graph-based approach. Use geodesic mode for curved manifolds where Euclidean distances are misleading.</li> </ul>"},{"location":"tutorials/comparing_estimators/","title":"Comparing Estimators","text":"<p>Different fields use different definitions of \"effective dimension\". This tutorial highlights the differences.</p>"},{"location":"tutorials/comparing_estimators/#pca-vs-participation-ratio","title":"PCA vs Participation Ratio","text":"<ul> <li>PCA relies on a hard threshold (e.g., 95% variance). It answers \"how many axes do I need to keep?\".</li> <li>Participation Ratio (PR) is a \"soft\" count. It answers \"how spread out is the variance?\".</li> </ul> <p>Consider a spectrum where eigenvalues decay slowly: \\(\\lambda_i = 1/i\\).</p> <pre><code>import numpy as np\nimport effdim\n\n# Simulate a slow decay spectrum directly\nD = 50\nlambdas = 1.0 / np.arange(1, D+1)\n\n# Generate data X (N=1000, D=50) that respects this spectrum\n# X = U * S * V.T\n# Singular values s_i = sqrt(lambda_i * (N-1))\nN = 1000\ns = np.sqrt(lambdas * (N - 1))\n# Random orthogonal matrix U (N x D)\nU, _ = np.linalg.qr(np.random.randn(N, D))\n\nX = U @ np.diag(s)\n\nresults = effdim.compute_dim(X)\npca_95 = results['pca_explained_variance_95']\npr = results['participation_ratio']\n\nprint(f\"PCA (95%): {pca_95}\")\nprint(f\"Participation Ratio: {pr:.2f}\")\n</code></pre> <p>In heavy-tailed distributions, PCA might suggest a very high dimension (to capture the tail), whereas PR might suggest a lower dimension because the mass is concentrated at the start.</p>"},{"location":"tutorials/comparing_estimators/#shannon-vs-renyi","title":"Shannon vs R\u00e9nyi","text":"<p>Shannon Entropy weights probabilities logarithmically. R\u00e9nyi entropy (with \\(\\alpha=2\\), which relates to PR) weights higher probabilities more heavily.</p> <ul> <li>Shannon is sensitive to the entire distribution.</li> <li>PR (R\u00e9nyi-2) is more dominated by the largest eigenvalues.</li> </ul> <p>If you have a dataset with many small noise directions, Shannon dimension might be higher than PR.</p>"},{"location":"tutorials/geometric_analysis/","title":"Geometric Analysis","text":"<p>Geometric estimators calculate the \"Intrinsic Dimension\" (ID) based on distances between points, rather than variance of global projections. This is crucial for manifolds that are non-linear (e.g., a Swiss Roll).</p>"},{"location":"tutorials/geometric_analysis/#the-swiss-roll-problem","title":"The Swiss Roll Problem","text":"<p>A \"Swiss Roll\" is a 2D plane rolled up in 3D.</p> <ul> <li>PCA will see it as 3D (because variance exists in x, y, z).</li> <li>Geometric ID should see it as 2D (locally, it's a plane).</li> </ul> <pre><code>import numpy as np\nimport effdim\nfrom sklearn.datasets import make_swiss_roll\n\n# Generate Swiss Roll\nX, _ = make_swiss_roll(n_samples=2000, noise=0.01)\n\n# Compute dimensionalities\nresults = effdim.compute_dim(X)\n\n# PCA\npca_dim = results['pca_explained_variance_95']\nprint(f\"Global PCA Dimension: {pca_dim}\")\n# Likely 3, because the roll occupies 3D volume globally.\n\n# kNN Intrinsic Dimension (MLE)\nknn_dim = results['mle_dimensionality']\nprint(f\"kNN Intrinsic Dimension: {knn_dim:.2f}\")\n# Should be close to 2.0\n\n# Two-NN\ntwonn_dim = results['two_nn_dimensionality']\nprint(f\"Two-NN Intrinsic Dimension: {twonn_dim:.2f}\")\n# Should be close to 2.0\n</code></pre>"},{"location":"tutorials/geometric_analysis/#when-to-use-geometric-estimators","title":"When to use Geometric Estimators?","text":"<ol> <li>Non-linear manifolds: Image datasets (digits, faces) often lie on low-dimensional non-linear manifolds.</li> <li>Manifold Learning: Checking if your autoencoder latent space has matched the intrinsic dimension of the data.</li> <li>Local Analysis: Using pure geometry approaches can capture local variability better.</li> </ol>"},{"location":"tutorials/geometric_analysis/#limitations","title":"Limitations","text":"<ul> <li>Computational Cost: Requires computing nearest neighbors, which can be slow for large \\(N\\). <code>effdim</code> utilizes the highly efficient CFaiss implementation under the hood to speed this up.</li> <li>Curse of Dimensionality: In extremely high dimensions, distance concentration can make geometric estimation unstable.</li> </ul>"},{"location":"tutorials/getting_started/","title":"Getting Started","text":"<p>This guide will walk you through the basic usage of <code>effdim</code>.</p>"},{"location":"tutorials/getting_started/#installation","title":"Installation","text":"<p>Ensure <code>effdim</code> is installed:</p> <pre><code>pip install effdim\n</code></pre>"},{"location":"tutorials/getting_started/#basic-concepts","title":"Basic Concepts","text":"<p>EffDim revolves around a single main function that computes various effective dimensionalities at once:</p> <ul> <li><code>effdim.compute_dim(data)</code>: Calculates a dictionary of dimension metrics.</li> </ul> <p>Data is typically passed as a N x D numpy array, where \\(N\\) is the number of samples and \\(D\\) is the number of features.</p>"},{"location":"tutorials/getting_started/#example-random-noise-vs-structured-data","title":"Example: Random Noise vs Structured Data","text":"<p>Let's see how effective dimension differs between random noise and structured data.</p>"},{"location":"tutorials/getting_started/#1-random-noise","title":"1. Random Noise","text":"<p>High-dimensional random noise should have a high effective dimension because the variance is spread out in all directions.</p> <pre><code>import numpy as np\nimport effdim\n\n# 1000 samples, 100 dimensions\nnoise = np.random.randn(1000, 100)\n\n# Compute dimensionalities\nresults = effdim.compute_dim(noise)\npr = results['participation_ratio']\nprint(f\"PR of Noise: {pr:.2f}\")\n# Expected: close to 100 (or slightly less due to finite sampling)\n</code></pre>"},{"location":"tutorials/getting_started/#2-structured-data-low-rank","title":"2. Structured Data (Low Rank)","text":"<p>If we create data that lies on a low-dimensional plane embedded in high-dimensional space, the effective dimension should be low.</p> <pre><code># Create 1000 samples with only 5 meaningful dimensions\nlatent = np.random.randn(1000, 5)\nprojection = np.random.randn(5, 100)\nstructured_data = latent @ projection\n\n# Add a tiny bit of noise\nstructured_data += 0.01 * np.random.randn(1000, 100)\n\npr = effdim.compute_dim(structured_data)['participation_ratio']\nprint(f\"PR of Structured Data: {pr:.2f}\")\n# Expected: close to 5\n</code></pre>"},{"location":"tutorials/getting_started/#available-methods","title":"Available Methods","text":"<p>You can check the available methods in the Theory section.</p> <p>Spectral Methods:</p> <ul> <li><code>pca_explained_variance_95</code>: PCA Explained Variance (with 95% threshold)</li> <li><code>participation_ratio</code>: Participation Ratio</li> <li><code>shannon_entropy</code>: Shannon Effective Dimensionality</li> <li><code>renyi_eff_dimensionality_alpha_2</code> (also 3, 4, 5): R\u00e9nyi Effective Dimensionality</li> <li><code>geometric_mean_eff_dimensionality</code>: Geometric Mean Dimension</li> </ul> <p>Geometric Methods:</p> <ul> <li><code>mle_dimensionality</code>: k-Nearest Neighbors (Maximum Likelihood Estimate)</li> <li><code>two_nn_dimensionality</code>: Two-Nearest Neighbors</li> </ul>"},{"location":"tutorials/getting_started/#analyzing-multiple-metrics","title":"analyzing Multiple Metrics","text":"<p>Use <code>effdim.compute_dim</code> to get a report with all available estimators at once.</p> <pre><code>report = effdim.compute_dim(structured_data)\nprint(report)\n# {'pca_explained_variance_95': ..., 'participation_ratio': ..., 'shannon_entropy': ..., 'mle_dimensionality': ..., ...}\n</code></pre>"}]}
{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Welcome to EffDim","text":"<p>EffDim is a unified, research-oriented Python library designed to compute \"effective dimensionality\" (ED) across diverse data modalities.</p> <p>It aims to standardize the fragmented landscape of ED metrics found in statistics, physics, information theory, and machine learning into a single, cohesive interface.</p> <p>Performance Enhancement</p> <p>EffDim now includes a Rust-accelerated implementation of geometry functions, providing 10-50x speedup for large datasets! Prebuilt wheels are available for all major platforms.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Modality Agnostic: Works with raw data, covariance matrices, and pre-computed spectra.</li> <li>Unified Interface: Simple <code>compute</code> and <code>analyze</code> functions.</li> <li>Extensive Estimators: PCA, Participation Ratio, Shannon Entropy, and more.</li> <li>Research Ready: Accurate implementations of metrics from literature.</li> <li>High Performance: Rust-accelerated geometry calculations for large-scale datasets.</li> </ul>"},{"location":"#installation","title":"Installation","text":"<p>Install via pip (includes prebuilt Rust extensions):</p> <pre><code>pip install effdim\n</code></pre> <p>Prebuilt wheels with Rust acceleration are available for:</p> <ul> <li>Linux (manylinux, x86_64 &amp; aarch64)</li> <li>macOS (x86_64 &amp; Apple Silicon ARM64)</li> <li>Windows (x86_64)</li> <li>Python versions: 3.8, 3.9, 3.10, 3.11, 3.12</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":"<pre><code>import numpy as np\nimport effdim\n\n# Generate random high-dimensional data\ndata = np.random.randn(100, 50)\n\n# Compute Effective Dimension using PCA (95% variance)\ned = effdim.compute(data, method='pca', threshold=0.95)\nprint(f\"Effective Dimension (PCA): {ed}\")\n\n# Compute using Participation Ratio\npr = effdim.compute(data, method='participation_ratio')\nprint(f\"Participation Ratio: {pr}\")\n</code></pre> <p>Explore the User Guide for more examples.</p>"},{"location":"api/","title":"API Reference","text":""},{"location":"api/#main-interface","title":"Main Interface","text":""},{"location":"api/#metrics-spectral","title":"Metrics (Spectral)","text":""},{"location":"api/#effdim.metrics.geometric_mean_eff_dimensionality","title":"<code>geometric_mean_eff_dimensionality(spectrum)</code>","text":"<p>Compute the Geometric Mean Effective Dimensionality of the given spectrum.</p>"},{"location":"api/#effdim.metrics.geometric_mean_eff_dimensionality--parameters","title":"Parameters:","text":"<p>spectrum : np.ndarray     Array of eigenvalues.</p>"},{"location":"api/#effdim.metrics.geometric_mean_eff_dimensionality--returns","title":"Returns:","text":"<p>float     Geometric Mean Effective Dimensionality value.</p> Source code in <code>src/effdim/metrics.py</code> <pre><code>def geometric_mean_eff_dimensionality(spectrum: np.ndarray) -&gt; float:\n    \"\"\"\n    Compute the Geometric Mean Effective Dimensionality of the given spectrum.\n\n    Parameters:\n    -----------\n    spectrum : np.ndarray\n        Array of eigenvalues.\n\n    Returns:\n    --------\n    float\n        Geometric Mean Effective Dimensionality value.\n    \"\"\"\n    positive_spectrum = spectrum[spectrum &gt; 0]\n    if len(positive_spectrum) == 0:\n        return 0.0\n\n    # Calculate the arthmetic mean of the positive spectrum\n    am = np.mean(positive_spectrum)\n    # Calculate the geometric mean of the positive spectrum\n    gm = np.exp(np.mean(np.log(positive_spectrum)))\n    d_eff = (am / gm)\n\n    return d_eff\n</code></pre>"},{"location":"api/#effdim.metrics.participation_ratio","title":"<code>participation_ratio(spectrum)</code>","text":"<p>Compute the Participation Ratio (PR) of the given spectrum.</p>"},{"location":"api/#effdim.metrics.participation_ratio--parameters","title":"Parameters:","text":"<p>spectrum : np.ndarray     Array of eigenvalues.</p>"},{"location":"api/#effdim.metrics.participation_ratio--returns","title":"Returns:","text":"<p>float     Participation Ratio value.</p> Source code in <code>src/effdim/metrics.py</code> <pre><code>def participation_ratio(spectrum: np.ndarray) -&gt; float:\n    \"\"\"\n    Compute the Participation Ratio (PR) of the given spectrum.\n\n    Parameters:\n    -----------\n    spectrum : np.ndarray\n        Array of eigenvalues.\n\n    Returns:\n    --------\n    float\n        Participation Ratio value.\n    \"\"\"\n    numerator = (np.sum(spectrum)) ** 2\n    denominator = np.sum(spectrum ** 2)\n    if denominator == 0:\n        return 0.0\n    return numerator / denominator\n</code></pre>"},{"location":"api/#effdim.metrics.pca_explained_variance","title":"<code>pca_explained_variance(spectrum, threshold=0.95)</code>","text":"<p>Compute the number of principal components required to explain a given threshold of variance.</p>"},{"location":"api/#effdim.metrics.pca_explained_variance--parameters","title":"Parameters:","text":"<p>spectrum : np.ndarray     Array of eigenvalues (explained variance) from PCA. threshold : float     The cumulative variance threshold to reach (between 0 and 1).</p>"},{"location":"api/#effdim.metrics.pca_explained_variance--returns","title":"Returns:","text":"<p>int     Number of principal components needed to reach the threshold.</p> Source code in <code>src/effdim/metrics.py</code> <pre><code>def pca_explained_variance(spectrum: np.ndarray, threshold: float = 0.95) -&gt; int:\n    \"\"\"\n    Compute the number of principal components required to explain a given\n    threshold of variance.\n\n    Parameters:\n    -----------\n    spectrum : np.ndarray\n        Array of eigenvalues (explained variance) from PCA.\n    threshold : float\n        The cumulative variance threshold to reach (between 0 and 1).\n\n    Returns:\n    --------\n    int\n        Number of principal components needed to reach the threshold.\n    \"\"\"\n    total_variance = np.sum(spectrum)\n    cumulative_variance = np.cumsum(spectrum)\n    explained_variance_ratio = cumulative_variance / total_variance\n\n    num_components = np.searchsorted(explained_variance_ratio, threshold) + 1\n    return num_components\n</code></pre>"},{"location":"api/#effdim.metrics.renyi_eff_dimensionality","title":"<code>renyi_eff_dimensionality(probabilities, alpha)</code>","text":"<p>Compute the R\u00e9nyi Effective Dimensionality of the given probability distribution.</p>"},{"location":"api/#effdim.metrics.renyi_eff_dimensionality--parameters","title":"Parameters:","text":"<p>probabilities : np.ndarray     Array of probabilities. alpha : float     Order of the R\u00e9nyi entropy (alpha &gt; 0 and alpha != 1).</p>"},{"location":"api/#effdim.metrics.renyi_eff_dimensionality--returns","title":"Returns:","text":"<p>float     R\u00e9nyi Effective Dimensionality value.</p> Source code in <code>src/effdim/metrics.py</code> <pre><code>def renyi_eff_dimensionality(probabilities: np.ndarray, alpha: float) -&gt; float:\n    \"\"\"\n    Compute the R\u00e9nyi Effective Dimensionality of the given probability distribution.\n\n    Parameters:\n    -----------\n    probabilities : np.ndarray\n        Array of probabilities.\n    alpha : float\n        Order of the R\u00e9nyi entropy (alpha &gt; 0 and alpha != 1).\n\n    Returns:\n    --------\n    float\n        R\u00e9nyi Effective Dimensionality value.\n    \"\"\"\n    if alpha &lt;= 0 or alpha == 1:\n        raise ValueError(\"Alpha must be greater than 0 and not equal to 1.\")\n\n    sum_probs_alpha = np.sum(probabilities ** alpha)\n    if sum_probs_alpha == 0:\n        return 0.0\n\n    d_eff = sum_probs_alpha ** (1 / (1 - alpha))\n    return d_eff\n</code></pre>"},{"location":"api/#effdim.metrics.shannon_entropy","title":"<code>shannon_entropy(probabilities)</code>","text":"<p>Compute the Shannon Entropy of the given probability distribution.</p>"},{"location":"api/#effdim.metrics.shannon_entropy--parameters","title":"Parameters:","text":"<p>probabilities : np.ndarray     Array of probabilities.</p>"},{"location":"api/#effdim.metrics.shannon_entropy--returns","title":"Returns:","text":"<p>float     Shannon Entropy value.</p> Source code in <code>src/effdim/metrics.py</code> <pre><code>def shannon_entropy(probabilities: np.ndarray) -&gt; float:\n    \"\"\"\n    Compute the Shannon Entropy of the given probability distribution.\n\n    Parameters:\n    -----------\n    probabilities : np.ndarray\n        Array of probabilities.\n\n    Returns:\n    --------\n    float\n        Shannon Entropy value.\n    \"\"\"\n    # Filter out zero probabilities to avoid log(0)\n    probabilities = probabilities[probabilities &gt; 0]\n    entropy = -np.sum(probabilities * np.log(probabilities))\n    d_eff = np.exp(entropy)\n    return d_eff\n</code></pre>"},{"location":"api/#geometry-spatial","title":"Geometry (Spatial)","text":"<p>Rust Acceleration</p> <p>The geometry functions (<code>mle_dimensionality</code>, <code>two_nn_dimensionality</code>, <code>box_counting_dimensionality</code>) automatically use a high-performance Rust implementation when available, providing 10-50x speedup for large datasets.</p>"},{"location":"api/#effdim.geometry.box_counting_dimensionality","title":"<code>box_counting_dimensionality(data, box_sizes=None)</code>","text":"<p>Estimate Box-Counting Dimension. Optimized loop and bounds calculation. Uses fast Rust implementation.</p> Source code in <code>src/effdim/geometry.py</code> <pre><code>def box_counting_dimensionality(data: np.ndarray, box_sizes: np.ndarray = None) -&gt; float:\n    \"\"\"\n    Estimate Box-Counting Dimension.\n    Optimized loop and bounds calculation.\n    Uses fast Rust implementation.\n    \"\"\"\n    if box_sizes is None:\n        # Auto-generate logarithmic box sizes if none provided\n        range_max = np.max(data) - np.min(data)\n        box_sizes = np.geomspace(range_max / 100, range_max / 5, num=10)\n\n    return _rust.box_counting_dimensionality(data, box_sizes)\n</code></pre>"},{"location":"api/#effdim.geometry.mle_dimensionality","title":"<code>mle_dimensionality(data, k=10)</code>","text":"<p>Estimate intrinsic dimensionality using Levina-Bickel MLE. Includes protection against duplicate points (distance=0). Uses fast Rust implementation.</p> Source code in <code>src/effdim/geometry.py</code> <pre><code>def mle_dimensionality(data: np.ndarray, k: int = 10) -&gt; float:\n    \"\"\"\n    Estimate intrinsic dimensionality using Levina-Bickel MLE.\n    Includes protection against duplicate points (distance=0).\n    Uses fast Rust implementation.\n    \"\"\"\n    return _rust.mle_dimensionality(data, k)\n</code></pre>"},{"location":"api/#effdim.geometry.two_nn_dimensionality","title":"<code>two_nn_dimensionality(data)</code>","text":"<p>Estimate intrinsic dimensionality using Two-NN. Corrects the regression target to -log(1 - F(mu)). Uses fast Rust implementation.</p> Source code in <code>src/effdim/geometry.py</code> <pre><code>def two_nn_dimensionality(data: np.ndarray) -&gt; float:\n    \"\"\"\n    Estimate intrinsic dimensionality using Two-NN.\n    Corrects the regression target to -log(1 - F(mu)).\n    Uses fast Rust implementation.\n    \"\"\"\n    return _rust.two_nn_dimensionality(data)\n</code></pre>"},{"location":"deployment/","title":"Deployment and Publishing","text":"<p>This guide is for maintainers who publish releases to PyPI.</p>"},{"location":"deployment/#overview","title":"Overview","text":"<p>EffDim uses GitHub Actions to automatically build and publish prebuilt wheels for multiple platforms and Python versions.</p>"},{"location":"deployment/#prerequisites","title":"Prerequisites","text":""},{"location":"deployment/#pypi-account-setup","title":"PyPI Account Setup","text":"<ol> <li>Create a PyPI account at pypi.org</li> <li>Generate an API token:</li> <li>Go to Account Settings \u2192 API Tokens</li> <li>Click \"Add API token\"</li> <li>Name: <code>effdim-github-actions</code></li> <li>Scope: <code>Project: effdim</code></li> <li>Copy the token (starts with <code>pypi-</code>)</li> </ol>"},{"location":"deployment/#github-repository-setup","title":"GitHub Repository Setup","text":"<ol> <li>Go to repository Settings \u2192 Secrets and variables \u2192 Actions</li> <li>Click New repository secret</li> <li>Name: <code>PYPI_API_TOKEN</code></li> <li>Value: [paste PyPI token]</li> <li>Click Add secret</li> </ol>"},{"location":"deployment/#release-process","title":"Release Process","text":""},{"location":"deployment/#1-update-version","title":"1. Update Version","text":"<p>Edit <code>pyproject.toml</code>:</p> <pre><code>[project]\nversion = \"0.1.1\"  # Update this line\n</code></pre>"},{"location":"deployment/#2-update-changelog","title":"2. Update Changelog","text":"<p>Document changes in <code>CHANGELOG.md</code> or release notes:</p> <pre><code>## [0.1.1] - 2024-01-23\n\n### Added\n- New feature X\n- Performance improvements\n\n### Fixed\n- Bug in function Y\n</code></pre>"},{"location":"deployment/#3-commit-changes","title":"3. Commit Changes","text":"<pre><code>git add pyproject.toml CHANGELOG.md\ngit commit -m \"Bump version to 0.1.1\"\ngit push origin main\n</code></pre>"},{"location":"deployment/#4-create-and-push-tag","title":"4. Create and Push Tag","text":"<pre><code># Create annotated tag\ngit tag -a v0.1.1 -m \"Release version 0.1.1\"\n\n# Push tag to trigger workflow\ngit push origin v0.1.1\n</code></pre>"},{"location":"deployment/#5-monitor-build","title":"5. Monitor Build","text":"<ol> <li>Go to Actions tab in GitHub</li> <li>Watch the \"Build and Publish to PyPI\" workflow</li> <li>Verify all jobs complete successfully:</li> <li>\u2705 Build wheels (Linux, macOS, Windows)</li> <li>\u2705 Build source distribution</li> <li>\u2705 Publish to PyPI</li> </ol>"},{"location":"deployment/#6-verify-release","title":"6. Verify Release","text":"<p>After successful workflow:</p> <pre><code># Wait a few minutes for PyPI to update\npip install --upgrade effdim\n\n# Verify version\npython -c \"import effdim; print(effdim.__version__)\"\n</code></pre>"},{"location":"deployment/#build-matrix","title":"Build Matrix","text":"<p>The CI workflow builds wheels for:</p>"},{"location":"deployment/#platforms-and-architectures","title":"Platforms and Architectures","text":""},{"location":"deployment/#linux","title":"Linux","text":"<ul> <li>manylinux: x86_64, aarch64</li> <li>musllinux: x86_64, aarch64</li> </ul>"},{"location":"deployment/#windows","title":"Windows","text":"<ul> <li>x64 (64-bit)</li> <li>x86 (32-bit)</li> </ul>"},{"location":"deployment/#macos","title":"macOS","text":"<ul> <li>x86_64 (Intel) - macOS 13+</li> <li>aarch64 (Apple Silicon) - macOS 14+</li> </ul>"},{"location":"deployment/#python-versions","title":"Python Versions","text":"<p>The workflow uses <code>--find-interpreter</code> to automatically build for all available Python versions (3.8-3.12) on each platform.</p>"},{"location":"deployment/#total-artifacts","title":"Total Artifacts","text":"<p>Approximately 40+ wheels + 1 source distribution per release, covering all combinations of platforms, architectures, and Python versions.</p>"},{"location":"deployment/#workflow-files","title":"Workflow Files","text":""},{"location":"deployment/#githubworkflowsciyml","title":"<code>.github/workflows/CI.yml</code>","text":"<p>The main CI/CD workflow based on maturin's recommended structure:</p> <ul> <li>Triggered by: Pushes to main/master, PRs, tags, or manual dispatch</li> <li>Separate jobs for: linux, musllinux, windows, macos, sdist, release</li> <li>Publishes: To PyPI on version tags (automatically)</li> </ul> <p>Key features:</p> <ul> <li>Uses PyO3/maturin-action</li> <li>Enables sccache for faster builds (disabled on release tags)</li> <li>Builds manylinux and musllinux for maximum compatibility</li> <li>Unique artifact naming prevents conflicts</li> <li>Includes build attestations for security</li> </ul>"},{"location":"deployment/#githubworkflowspublish_docsyml","title":"<code>.github/workflows/publish_docs.yml</code>","text":"<p>Publishes documentation to GitHub Pages (unchanged).</p>"},{"location":"deployment/#testing-before-release","title":"Testing Before Release","text":""},{"location":"deployment/#option-1-manual-workflow-trigger","title":"Option 1: Manual Workflow Trigger","text":"<ol> <li>Go to Actions \u2192 Build and Publish to PyPI</li> <li>Click Run workflow</li> <li>Select branch</li> <li>Review artifacts (won't publish without tag)</li> </ol>"},{"location":"deployment/#option-2-test-with-testpypi","title":"Option 2: Test with TestPyPI","text":"<p>Modify workflow temporarily to use TestPyPI:</p> <pre><code>- name: Publish to TestPyPI\n  uses: PyO3/maturin-action@v1\n  env:\n    MATURIN_PYPI_TOKEN: ${{ secrets.TEST_PYPI_API_TOKEN }}\n  with:\n    command: upload\n    args: --non-interactive --repository-url https://test.pypi.org/legacy/ dist/*\n</code></pre> <p>Then install from TestPyPI:</p> <pre><code>pip install --index-url https://test.pypi.org/simple/ --extra-index-url https://pypi.org/simple effdim\n</code></pre>"},{"location":"deployment/#versioning-strategy","title":"Versioning Strategy","text":"<p>EffDim follows Semantic Versioning:</p> <ul> <li>MAJOR (0.x.x \u2192 1.x.x): Breaking API changes</li> <li>MINOR (x.1.x \u2192 x.2.x): New features, backwards compatible</li> <li>PATCH (x.x.1 \u2192 x.x.2): Bug fixes, backwards compatible</li> </ul>"},{"location":"deployment/#pre-releases","title":"Pre-releases","text":"<p>For beta/RC versions:</p> <pre><code>version = \"0.2.0b1\"  # Beta 1\nversion = \"0.2.0rc1\" # Release Candidate 1\n</code></pre> <p>Tag as: <code>v0.2.0b1</code>, <code>v0.2.0rc1</code></p>"},{"location":"deployment/#troubleshooting","title":"Troubleshooting","text":""},{"location":"deployment/#build-failures","title":"Build Failures","text":"<p>Rust compilation errors:</p> <pre><code># Check locally\nmaturin build --release\n\n# View logs in GitHub Actions\n</code></pre> <p>Python compatibility issues:</p> <ul> <li>Ensure <code>requires-python</code> in <code>pyproject.toml</code> matches tested versions</li> <li>Check minimum Rust version in <code>Cargo.toml</code></li> </ul>"},{"location":"deployment/#upload-failures","title":"Upload Failures","text":"<p>Invalid token:</p> <ul> <li>Regenerate PyPI token</li> <li>Update <code>PYPI_API_TOKEN</code> secret in GitHub</li> </ul> <p>Package name conflict:</p> <ul> <li>First release must be manually created on PyPI</li> <li>Or use different package name</li> </ul> <p>File already exists:</p> <ul> <li>Can't re-upload same version</li> <li>Bump version and retry</li> <li>Use <code>--skip-existing</code> flag (already in workflow)</li> </ul>"},{"location":"deployment/#workflow-not-triggering","title":"Workflow Not Triggering","text":"<p>Tag format issues:</p> <pre><code># Correct\ngit tag v0.1.1\n\n# Incorrect\ngit tag 0.1.1  # Missing 'v' prefix\n</code></pre> <p>Branch protection:</p> <ul> <li>Ensure tags can be pushed to repository</li> <li>Check branch protection rules</li> </ul>"},{"location":"deployment/#rollback-procedure","title":"Rollback Procedure","text":"<p>If a bad release is published:</p>"},{"location":"deployment/#option-1-yank-release-recommended","title":"Option 1: Yank Release (Recommended)","text":"<p>On PyPI:</p> <ol> <li>Go to project page</li> <li>Click on problematic version</li> <li>Click \"Options\" \u2192 \"Yank release\"</li> <li>Publish fixed version</li> </ol>"},{"location":"deployment/#option-2-delete-release","title":"Option 2: Delete Release","text":"<p>\u26a0\ufe0f Not recommended - breaks existing installations</p> <pre><code># Delete tag locally\ngit tag -d v0.1.1\n\n# Delete tag remotely\ngit push origin :refs/tags/v0.1.1\n</code></pre> <p>Then publish corrected version.</p>"},{"location":"deployment/#security","title":"Security","text":""},{"location":"deployment/#api-token-management","title":"API Token Management","text":"<ul> <li>Rotate tokens periodically (every 6-12 months)</li> <li>Use project-scoped tokens (not account-wide)</li> <li>Never commit tokens to repository</li> <li>Store in GitHub Secrets only</li> </ul>"},{"location":"deployment/#dependency-security","title":"Dependency Security","text":"<p>Automated security scanning:</p> <ul> <li>Dependabot alerts (GitHub)</li> <li><code>cargo audit</code> for Rust dependencies</li> <li><code>pip-audit</code> for Python dependencies</li> </ul>"},{"location":"deployment/#maintenance-checklist","title":"Maintenance Checklist","text":"<p>Before each release:</p> <ul> <li>[ ] All tests passing</li> <li>[ ] Documentation updated</li> <li>[ ] Changelog updated</li> <li>[ ] Version bumped</li> <li>[ ] Dependencies updated</li> <li>[ ] Security audit clean</li> <li>[ ] Performance benchmarks run</li> <li>[ ] Breaking changes documented</li> </ul>"},{"location":"deployment/#additional-resources","title":"Additional Resources","text":"<ul> <li>Maturin Documentation</li> <li>PyPI Help</li> <li>GitHub Actions Documentation</li> <li>Semantic Versioning</li> </ul>"},{"location":"performance/","title":"Performance and Rust Implementation","text":"<p>EffDim includes a high-performance Rust implementation of geometry-based dimensionality estimators, providing significant speedups for large datasets.</p>"},{"location":"performance/#overview","title":"Overview","text":"<p>The Rust implementation uses:</p> <ul> <li>Parallel brute-force nearest neighbor search with rayon</li> <li>Optimized for high-dimensional data (100-1000+ dimensions)</li> <li>Automatic fallback to Python implementation if unavailable</li> <li>Multi-core parallelization for maximum performance</li> </ul>"},{"location":"performance/#performance-benchmarks","title":"Performance Benchmarks","text":"<p>Benchmark results on GitHub Actions runners (4 CPU cores):</p> Samples Dimensions MLE Time (Rust) Two-NN Time (Rust) Speedup vs Python 1,000 100 0.05s 0.05s ~10x 5,000 200 2.5s 2.5s ~30x 10,000 700 36s 36s ~50x <p>Scaling</p> <p>Performance scales roughly linearly with sample count and quadratically with the number of nearest neighbors (k).</p>"},{"location":"performance/#why-brute-force","title":"Why Brute-Force?","text":"<p>Traditional k-d trees perform poorly in high dimensions due to the curse of dimensionality. For data with 100+ dimensions:</p> <ul> <li>K-d trees: Performance degrades to O(n\u00b2) anyway</li> <li>Brute-force + SIMD + Parallelization: Consistent O(n\u00b2) but with much better constants</li> <li>Result: Brute-force is faster for high-dimensional data</li> </ul>"},{"location":"performance/#affected-functions","title":"Affected Functions","text":"<p>The following functions use the Rust implementation:</p> <ul> <li><code>geometry.mle_dimensionality()</code> - MLE (Levina-Bickel) estimator</li> <li><code>geometry.two_nn_dimensionality()</code> - Two-NN (Facco et al.) estimator</li> <li><code>geometry.box_counting_dimensionality()</code> - Box-counting dimension</li> </ul>"},{"location":"performance/#installation","title":"Installation","text":""},{"location":"performance/#prebuilt-wheels-recommended","title":"Prebuilt Wheels (Recommended)","text":"<pre><code>pip install effdim\n</code></pre> <p>The Rust extension is automatically included in prebuilt wheels for Linux, macOS, and Windows.</p>"},{"location":"performance/#building-from-source","title":"Building from Source","text":"<p>If prebuilt wheels aren't available for your platform:</p> <pre><code># Install Rust\ncurl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh\n\n# Install maturin\npip install maturin\n\n# Build and install\nmaturin build --release\npip install target/wheels/effdim-*.whl\n</code></pre> <p>See the repository README for detailed build instructions.</p>"},{"location":"performance/#checking-rust-availability","title":"Checking Rust Availability","text":"<p>You can check if the Rust implementation is available:</p> <pre><code>from effdim import geometry\n\nif geometry._RUST_AVAILABLE:\n    print(\"Using fast Rust implementation!\")\nelse:\n    print(\"Using Python fallback\")\n</code></pre>"},{"location":"performance/#cpu-core-utilization","title":"CPU Core Utilization","text":"<p>The Rust implementation automatically uses all available CPU cores. Performance scales with:</p> <ul> <li>Number of cores: Near-linear scaling up to ~8 cores</li> <li>CPU speed: Single-threaded performance matters</li> <li>Cache size: Larger L2/L3 cache helps with large datasets</li> </ul> <p>Performance Tips</p> <ul> <li>Use release builds (<code>maturin build --release</code>) for maximum performance</li> <li>Ensure your system has sufficient RAM (dataset size \u00d7 8 bytes)</li> <li>For very large datasets (1M+ samples), consider batch processing</li> </ul>"},{"location":"performance/#technical-details","title":"Technical Details","text":""},{"location":"performance/#dependencies","title":"Dependencies","text":"<p>The Rust implementation uses:</p> <ul> <li>PyO3 (0.23) - Python bindings</li> <li>numpy (0.23) - NumPy array interop</li> <li>ndarray (0.16) - N-dimensional arrays</li> <li>rayon (1.10) - Data parallelism</li> </ul>"},{"location":"performance/#algorithm-details","title":"Algorithm Details","text":""},{"location":"performance/#mle-dimensionality","title":"MLE Dimensionality","text":"<ol> <li>Build point collection from input data</li> <li>For each point (in parallel):</li> <li>Find k+1 nearest neighbors using brute-force search</li> <li>Calculate distance ratios</li> <li>Compute local dimension estimate</li> <li>Return mean of local estimates</li> </ol>"},{"location":"performance/#two-nn-dimensionality","title":"Two-NN Dimensionality","text":"<ol> <li>Build point collection from input data</li> <li>For each point (in parallel):</li> <li>Find 3 nearest neighbors (self + 2)</li> <li>Calculate \u03bc = r\u2082/r\u2081 ratio</li> <li>Perform linear regression on sorted ratios</li> <li>Return dimension estimate</li> </ol>"},{"location":"performance/#troubleshooting","title":"Troubleshooting","text":""},{"location":"performance/#import-errors","title":"Import Errors","text":"<p>If you see <code>ModuleNotFoundError: No module named 'effdim._rust'</code>:</p> <ol> <li>Ensure you're using a prebuilt wheel or built from source</li> <li>Check Python version compatibility (3.8-3.12)</li> <li>Verify platform compatibility (Linux/macOS/Windows)</li> </ol>"},{"location":"performance/#performance-issues","title":"Performance Issues","text":"<p>If performance is slower than expected:</p> <ol> <li>Verify Rust implementation is being used: <code>geometry._RUST_AVAILABLE</code></li> <li>Check CPU usage - should use all cores</li> <li>Ensure release build (not debug)</li> <li>Monitor memory usage - swapping will slow everything down</li> </ol>"},{"location":"performance/#build-failures","title":"Build Failures","text":"<p>If building from source fails:</p> <ol> <li>Update Rust: <code>rustup update stable</code></li> <li>Install build dependencies: <code>pip install maturin setuptools-rust</code></li> <li>Check error logs in GitHub Actions/local build</li> <li>See RUST_BUILD.md for detailed troubleshooting</li> </ol>"},{"location":"performance/#contributing","title":"Contributing","text":"<p>Interested in improving performance? Contributions welcome!</p> <ul> <li>Benchmark new algorithms</li> <li>Optimize existing code</li> <li>Add GPU acceleration</li> <li>Improve documentation</li> </ul>"},{"location":"theory/","title":"Theory &amp; Estimators","text":"<p>EffDim implements a variety of estimators for \"effective dimensionality\" (ED). These can be broadly categorized into Spectral Estimators, which operate on the eigenvalues (spectrum) of the data's covariance/correlation matrix, and Geometric Estimators, which operate on the distances between data points.</p>"},{"location":"theory/#spectral-estimators","title":"Spectral Estimators","text":"<p>These methods rely on the spectrum \\(\\lambda_1 \\ge \\lambda_2 \\ge \\dots \\ge \\lambda_D \\ge 0\\) of the covariance matrix (or the squared singular values of the data matrix). We define the normalized spectrum as \\(p_i = \\frac{\\lambda_i}{\\sum_j \\lambda_j}\\), which can be treated as a probability distribution.</p>"},{"location":"theory/#pca-explained-variance","title":"PCA Explained Variance","text":"<p>The classic approach used in Principal Component Analysis. It defines the effective dimension as the number of components required to explain a certain fraction (threshold) of the total variance.</p> \\[ ED_{PCA}(x) = \\min \\{ k \\mid \\frac{\\sum_{i=1}^k \\lambda_i}{\\sum_{j=1}^D \\lambda_j} \\ge x \\} \\] <p>where \\(x\\) is the threshold (default 0.95).</p>"},{"location":"theory/#participation-ratio-pr","title":"Participation Ratio (PR)","text":"<p>Widely used in physics and neuroscience to quantify the \"spread\" of the spectrum. If the variance is equally distributed across \\(N\\) dimensions, \\(PR=N\\). If it is concentrated in 1 dimension, \\(PR=1\\).</p> \\[ PR = \\frac{(\\sum_i \\lambda_i)^2}{\\sum_i \\lambda_i^2} = \\frac{1}{\\sum_i p_i^2} \\]"},{"location":"theory/#shannon-effective-dimension","title":"Shannon Effective Dimension","text":"<p>Based on the Shannon entropy of the spectral distribution. It corresponds to the exponential of the entropy.</p> \\[ H = - \\sum_i p_i \\ln p_i $$ $$ ED_{Shannon} = \\exp(H) \\]"},{"location":"theory/#renyi-effective-dimension-alpha-entropy","title":"R\u00e9nyi Effective Dimension (Alpha-Entropy)","text":"<p>A generalization of the Shannon dimension using R\u00e9nyi entropy of order \\(\\alpha\\).</p> \\[ H_\\alpha = \\frac{1}{1-\\alpha} \\ln (\\sum_i p_i^\\alpha) $$ $$ ED_{\\alpha} = \\exp(H_\\alpha) \\] <ul> <li>For \\(\\alpha \\to 1\\), this converges to Shannon Effective Dimension.</li> <li>For \\(\\alpha = 2\\), this is equivalent to the Participation Ratio.</li> </ul>"},{"location":"theory/#effective-rank","title":"Effective Rank","text":"<p>Often used in matrix completion and low-rank approximation contexts. EffDim implements this as an alias for Shannon Effective Dimension.</p>"},{"location":"theory/#geometric-mean-dimension","title":"Geometric Mean Dimension","text":"<p>A dimension proxy based on the ratio of the arithmetic mean to the geometric mean of the spectrum.</p> \\[ d \\approx \\frac{\\frac{1}{D} \\sum \\lambda_i}{(\\prod \\lambda_i)^{1/D}} \\]"},{"location":"theory/#stable-rank","title":"Stable Rank","text":"<p>A stable alternative to the algebraic rank, often used in high-dimensional probability. It is robust to small perturbations of the singular values.</p> \\[ R_{stable} = \\frac{\\sum_i \\lambda_i}{\\max_i \\lambda_i} \\] <p>where \\(\\lambda_i\\) are the eigenvalues (variances).</p>"},{"location":"theory/#numerical-rank-epsilon-rank","title":"Numerical Rank (Epsilon-Rank)","text":"<p>The number of singular values greater than a specific threshold \\(\\epsilon\\).</p> \\[ rank_\\epsilon(A) = | \\{ \\sigma_i \\mid \\sigma_i &gt; \\epsilon \\} | \\] <p>If \\(\\epsilon\\) is not provided, it defaults to a value based on the machine precision and the largest singular value.</p>"},{"location":"theory/#cumulative-eigenvalue-ratio-cer","title":"Cumulative Eigenvalue Ratio (CER)","text":"<p>A weighted sum of the normalized spectrum, giving more weight to earlier components.</p> \\[ CER = \\sum_{i=1}^D w_i p_i \\] <p>where weights decrease linearly from 1 to 0.</p>"},{"location":"theory/#geometric-estimators","title":"Geometric Estimators","text":"<p>These methods estimate the intrinsic dimension (ID) of the data manifold based on local neighborhoods, without relying on global projections like PCA.</p>"},{"location":"theory/#knn-intrinsic-dimension-mle","title":"kNN Intrinsic Dimension (MLE)","text":"<p>The Maximum Likelihood Estimator proposed by Levina and Bickel (2005). It estimates dimension by examining the ratio of distances to the \\(k\\)-th nearest neighbor.</p> \\[ \\hat{d}_k(x_i) = \\left[ \\frac{1}{k-1} \\sum_{j=1}^{k-1} \\ln \\frac{r_k(x_i)}{r_j(x_i)} \\right]^{-1} \\] <p>where \\(r_j(x_i)\\) is the distance from \\(x_i\\) to its \\(j\\)-th nearest neighbor. The final estimate is the average over all points \\(x_i\\).</p>"},{"location":"theory/#two-nn","title":"Two-NN","text":"<p>A robust estimator proposed by Facco et al. (2017) that relies only on the distances to the first two nearest neighbors. It is less sensitive to density variations and curvature than standard kNN.</p> <p>It assumes that the ratio of distances \\(\\mu_i = \\frac{r_2(x_i)}{r_1(x_i)}\\) follows a Pareto distribution depending on the intrinsic dimension \\(d\\).</p>"},{"location":"theory/#danco","title":"DANCo","text":"<p>Dimensionality from Angle and Norm Concentration. This method jointly exploits the statistics of the norms of vectors to nearest neighbors and the angles between them. High-dimensional data exhibits specific concentration of measure properties for both angles and norms. DANCo estimates \\(d\\) by minimizing the KL-divergence between the empirical distributions and the theoretical distributions derived for a d-dimensional ball.</p>"},{"location":"theory/#mind-maximum-likelihood-on-minimum-distances","title":"MiND (Maximum Likelihood on Minimum Distances)","text":"<p>A family of estimators based on the statistics of nearest neighbor distances.</p> <ul> <li>MiND-MLi: Uses the distribution of the distance to the nearest neighbor (\\(r_1\\)).</li> <li>MiND-MLk: Uses the joint distribution of distances to the first \\(k\\) neighbors.</li> </ul>"},{"location":"theory/#ess-expected-simplex-skewness","title":"ESS (Expected Simplex Skewness)","text":"<p>Estimates dimension by analyzing the \"skewness\" (volume) of the simplex formed by a point and its neighbors. In high dimensions, random simplices tend to be regular (perfectly \"skewed\"). The estimator compares the empirical volumes of local simplices to theoretical expected volumes.</p>"},{"location":"theory/#tle-tight-localities-estimator","title":"TLE (Tight Localities Estimator)","text":"<p>Estimates dimension by maximizing the likelihood of distances within small, \"tight\" neighborhoods. It is designed to be robust to scale variations.</p>"},{"location":"theory/#gmst-geodesic-minimum-spanning-tree","title":"GMST (Geodesic Minimum Spanning Tree)","text":"<p>Estimates dimension based on the scaling law of the length of the Minimum Spanning Tree (MST) of a graph constructed from the data. $$ L(N) \\propto N^{1 - 1/d} $$ where \\(L(N)\\) is the length of the MST on \\(N\\) points. The dimension \\(d\\) is estimated from the slope of \\(\\log L(N)\\) vs \\(\\log N\\) using subsampling. The graph can be constructed using Euclidean distances or Geodesic distances (approximated by k-NN graph paths).</p>"},{"location":"tutorials/advanced_geometric_analysis/","title":"Advanced Geometric Analysis","text":"<p>EffDim provides a suite of advanced geometric estimators for Intrinsic Dimension (ID) estimation. These methods go beyond standard kNN/TwoNN approaches by exploiting additional statistical properties of high-dimensional space, such as the distribution of angles (DANCo) or simplex volumes (ESS).</p>"},{"location":"tutorials/advanced_geometric_analysis/#methods-overview","title":"Methods Overview","text":"Method Full Name Key Idea DANCo Dimensionality from Angle and Norm Concentration Uses both distances and angles to neighbors. MiND Minimum Neighbor Distance MLE based on min distances (MLi) or joint k-NN (MLk). ESS Expected Simplex Skewness Uses the volume of the simplex formed by neighbors. TLE Tight Localities Estimator Maximizes likelihood on scale-normalized distances. GMST Geodesic Minimum Spanning Tree Uses the scaling of MST length with sample size."},{"location":"tutorials/advanced_geometric_analysis/#setup","title":"Setup","text":"<p>We will generate two datasets:</p> <ol> <li>High-Dimensional Noise: 1000 points in 20D (ID should be ~20).</li> <li>Low-Dimensional Manifold: A 5D hypercube embedded in 20D space (ID should be ~5).</li> </ol> <pre><code>import numpy as np\nimport effdim\n\nnp.random.seed(42)\n\n# 1. High-Dimensional Noise (ID = 20)\nN = 1000\nD = 20\nnoise_data = np.random.randn(N, D)\n\n# 2. 5D Manifold embedded in 20D\n# Generate 5D data\nlatent = np.random.rand(N, 5)\n# Embed in 20D with a random rotation\nprojection = np.linalg.qr(np.random.randn(20, 5))[0]\nmanifold_data = latent @ projection.T\n</code></pre>"},{"location":"tutorials/advanced_geometric_analysis/#running-estimators","title":"Running Estimators","text":""},{"location":"tutorials/advanced_geometric_analysis/#danco","title":"DANCo","text":"<p>DANCo is often more accurate than kNN for high dimensions but is computationally more expensive due to angle computations.</p> <pre><code># Noise Data (Expect ~20)\nd_danco = effdim.compute(noise_data, method='danco')\nprint(f\"DANCo (Noise): {d_danco:.2f}\")\n\n# Manifold Data (Expect ~5)\nd_danco_m = effdim.compute(manifold_data, method='danco')\nprint(f\"DANCo (Manifold): {d_danco_m:.2f}\")\n</code></pre>"},{"location":"tutorials/advanced_geometric_analysis/#mind-mli-and-mlk","title":"MiND (MLi and MLk)","text":"<p>MiND estimators are fast and robust Maximum Likelihood approaches.</p> <pre><code># MiND-MLi (Uses 1st NN)\nd_mli = effdim.compute(manifold_data, method='mind_mli')\nprint(f\"MiND-MLi: {d_mli:.2f}\")\n\n# MiND-MLk (Uses k NNs)\nd_mlk = effdim.compute(manifold_data, method='mind_mlk', k=10)\nprint(f\"MiND-MLk: {d_mlk:.2f}\")\n</code></pre>"},{"location":"tutorials/advanced_geometric_analysis/#ess-expected-simplex-skewness","title":"ESS (Expected Simplex Skewness)","text":"<p>ESS compares the volume of local simplices to theoretical expectations. It relies on precomputed constants (currently supported for d=1 to 20).</p> <pre><code>d_ess = effdim.compute(manifold_data, method='ess')\nprint(f\"ESS: {d_ess:.2f}\")\n</code></pre>"},{"location":"tutorials/advanced_geometric_analysis/#gmst-geodesic-mst","title":"GMST (Geodesic MST)","text":"<p>GMST estimates dimension from the scaling of the Minimum Spanning Tree length. It is particularly useful for detecting the topology of the manifold.</p> <ul> <li><code>mode='euclidean'</code> (default): Uses Euclidean distances. Good for flat manifolds.</li> <li><code>mode='geodesic'</code>: Uses graph geodesic distances. Better for curved manifolds (like Swiss Roll).</li> </ul> <pre><code># Euclidean MST\nd_gmst = effdim.compute(manifold_data, method='gmst', mode='euclidean')\nprint(f\"GMST (Euclidean): {d_gmst:.2f}\")\n\n# For a curved manifold (e.g., Swiss Roll), use geodesic:\n# d_gmst_geo = effdim.compute(curved_data, method='gmst', mode='geodesic')\n</code></pre>"},{"location":"tutorials/advanced_geometric_analysis/#comparative-analysis","title":"Comparative Analysis","text":"<p>You can analyze multiple methods at once to check for consistency.</p> <pre><code>report = effdim.analyze(manifold_data, methods=['knn', 'twonn', 'danco', 'ess', 'mind_mlk'])\nfor method, val in report.items():\n    print(f\"{method}: {val:.2f}\")\n</code></pre>"},{"location":"tutorials/advanced_geometric_analysis/#limitations","title":"Limitations","text":"<ul> <li>Computational Cost: DANCo and GMST (geodesic) can be slow for large \\(N\\) (\\(&gt;10,000\\)).</li> <li>Sample Size: Geometric estimators generally require samples growing exponentially with dimension. For ID &gt; 20, estimates might degrade unless \\(N\\) is very large.</li> <li>ESS Constraints: ESS in <code>effdim</code> currently supports dimensions up to 20 due to precomputed constants.</li> </ul>"},{"location":"tutorials/comparing_estimators/","title":"Comparing Estimators","text":"<p>Different fields use different definitions of \"effective dimension\". This tutorial highlights the differences.</p>"},{"location":"tutorials/comparing_estimators/#pca-vs-participation-ratio","title":"PCA vs Participation Ratio","text":"<ul> <li>PCA relies on a hard threshold (e.g., 95% variance). It answers \"how many axes do I need to keep?\".</li> <li>Participation Ratio (PR) is a \"soft\" count. It answers \"how spread out is the variance?\".</li> </ul> <p>Consider a spectrum where eigenvalues decay slowly: \\(\\lambda_i = 1/i\\).</p> <pre><code>import numpy as np\nimport effdim\n\n# Simulate a slow decay spectrum directly\nD = 50\nlambdas = 1.0 / np.arange(1, D+1)\n\n# Generate data X (N=1000, D=50) that respects this spectrum\n# X = U * S * V.T\n# Singular values s_i = sqrt(lambda_i * (N-1))\nN = 1000\ns = np.sqrt(lambdas * (N - 1))\n# Random orthogonal matrix U (N x D)\nU, _ = np.linalg.qr(np.random.randn(N, D))\n\nX = U @ np.diag(s)\n\npca_95 = effdim.compute(X, method='pca', threshold=0.95)\npr = effdim.compute(X, method='pr')\n\nprint(f\"PCA (95%): {pca_95}\")\nprint(f\"Participation Ratio: {pr:.2f}\")\n</code></pre> <p>In heavy-tailed distributions, PCA might suggest a very high dimension (to capture the tail), whereas PR might suggest a lower dimension because the mass is concentrated at the start.</p>"},{"location":"tutorials/comparing_estimators/#shannon-vs-renyi","title":"Shannon vs R\u00e9nyi","text":"<p>Shannon Entropy weights probabilities logarithmically. R\u00e9nyi entropy (with \\(\\alpha=2\\), which relates to PR) weights higher probabilities more heavily.</p> <ul> <li>Shannon is sensitive to the entire distribution.</li> <li>PR (R\u00e9nyi-2) is more dominated by the largest eigenvalues.</li> </ul> <p>If you have a dataset with many small noise directions, Shannon dimension might be higher than PR.</p>"},{"location":"tutorials/geometric_analysis/","title":"Geometric Analysis","text":"<p>Geometric estimators calculate the \"Intrinsic Dimension\" (ID) based on distances between points, rather than variance of global projections. This is crucial for manifolds that are non-linear (e.g., a Swiss Roll).</p>"},{"location":"tutorials/geometric_analysis/#the-swiss-roll-problem","title":"The Swiss Roll Problem","text":"<p>A \"Swiss Roll\" is a 2D plane rolled up in 3D.</p> <ul> <li>PCA will see it as 3D (because variance exists in x, y, z).</li> <li>Geometric ID should see it as 2D (locally, it's a plane).</li> </ul> <pre><code>import numpy as np\nimport effdim\nfrom sklearn.datasets import make_swiss_roll\n\n# Generate Swiss Roll\nX, _ = make_swiss_roll(n_samples=2000, noise=0.01)\n\n# PCA\npca_dim = effdim.compute(X, method='pca', threshold=0.95)\nprint(f\"Global PCA Dimension: {pca_dim}\")\n# Likely 3, because the roll occupies 3D volume globally.\n\n# kNN Intrinsic Dimension\nknn_dim = effdim.compute(X, method='knn', k=5)\nprint(f\"kNN Intrinsic Dimension: {knn_dim:.2f}\")\n# Should be close to 2.0\n\n# Two-NN\ntwonn_dim = effdim.compute(X, method='twonn')\nprint(f\"Two-NN Intrinsic Dimension: {twonn_dim:.2f}\")\n# Should be close to 2.0\n</code></pre>"},{"location":"tutorials/geometric_analysis/#when-to-use-geometric-estimators","title":"When to use Geometric Estimators?","text":"<ol> <li>Non-linear manifolds: Image datasets (digits, faces) often lie on low-dimensional non-linear manifolds.</li> <li>Manifold Learning: Checking if your autoencoder latent space has matched the intrinsic dimension of the data.</li> <li>Local Analysis: kNN can be computed per-point (though <code>effdim</code> currently returns the average).</li> </ol>"},{"location":"tutorials/geometric_analysis/#limitations","title":"Limitations","text":"<ul> <li> <p>Computational Cost: Requires computing nearest neighbors, which can be slow for large \\(N\\).</p> <p>Performance</p> <p><code>effdim</code> uses a Rust-accelerated implementation with parallel nearest neighbor search for 10-50x speedup on large datasets. See Performance for benchmarks.</p> </li> <li> <p>Curse of Dimensionality: In extremely high dimensions, distance concentration can make geometric estimation unstable.</p> </li> </ul>"},{"location":"tutorials/getting_started/","title":"Getting Started","text":"<p>This guide will walk you through the basic usage of <code>effdim</code>.</p>"},{"location":"tutorials/getting_started/#installation","title":"Installation","text":"<p>Ensure <code>effdim</code> is installed:</p> <pre><code>pip install effdim\n</code></pre> <p>Performance</p> <p>The installed package includes prebuilt Rust extensions for high-performance geometry calculations. No additional setup needed!</p>"},{"location":"tutorials/getting_started/#basic-concepts","title":"Basic Concepts","text":"<p>EffDim revolves around two main functions:</p> <ul> <li><code>effdim.compute(data, method=...)</code>: Calculates a single dimension metric.</li> <li><code>effdim.analyze(data, methods=[...])</code>: Calculates multiple metrics at once.</li> </ul> <p>Data is typically passed as a N x D numpy array, where \\(N\\) is the number of samples and \\(D\\) is the number of features.</p>"},{"location":"tutorials/getting_started/#example-random-noise-vs-structured-data","title":"Example: Random Noise vs Structured Data","text":"<p>Let's see how effective dimension differs between random noise and structured data.</p>"},{"location":"tutorials/getting_started/#1-random-noise","title":"1. Random Noise","text":"<p>High-dimensional random noise should have a high effective dimension because the variance is spread out in all directions.</p> <pre><code>import numpy as np\nimport effdim\n\n# 1000 samples, 100 dimensions\nnoise = np.random.randn(1000, 100)\n\n# Participation Ratio\npr = effdim.compute(noise, method='participation_ratio')\nprint(f\"PR of Noise: {pr:.2f}\")\n# Expected: close to 100 (or slightly less due to finite sampling)\n</code></pre>"},{"location":"tutorials/getting_started/#2-structured-data-low-rank","title":"2. Structured Data (Low Rank)","text":"<p>If we create data that lies on a low-dimensional plane embedded in high-dimensional space, the effective dimension should be low.</p> <pre><code># Create 1000 samples with only 5 meaningful dimensions\nlatent = np.random.randn(1000, 5)\nprojection = np.random.randn(5, 100)\nstructured_data = latent @ projection\n\n# Add a tiny bit of noise\nstructured_data += 0.01 * np.random.randn(1000, 100)\n\npr = effdim.compute(structured_data, method='participation_ratio')\nprint(f\"PR of Structured Data: {pr:.2f}\")\n# Expected: close to 5\n</code></pre>"},{"location":"tutorials/getting_started/#available-methods","title":"Available Methods","text":"<p>You can check the available methods in the Theory section.</p> <p>Spectral Methods:</p> <ul> <li><code>'pca'</code>: PCA Explained Variance</li> <li><code>'participation_ratio'</code> (or <code>'pr'</code>)</li> <li><code>'shannon'</code> (or <code>'entropy'</code>)</li> <li><code>'effective_rank'</code> (or <code>'erank'</code>): Alias for Shannon Effective Dimension (Trace Norm).</li> <li><code>'renyi'</code></li> <li><code>'stable_rank'</code>: Ratio of sum/max eigenvalues.</li> <li><code>'numerical_rank'</code>: Count of singular values &gt; epsilon.</li> </ul> <p>Geometric Methods:</p> <ul> <li><code>'knn'</code>: k-Nearest Neighbors</li> <li><code>'twonn'</code>: Two-Nearest Neighbors</li> <li><code>'danco'</code>: Angle and Norm Concentration</li> <li><code>'mind_mlk'</code>: MiND (Maximum Likelihood)</li> <li><code>'ess'</code>: Expected Simplex Skewness</li> </ul>"},{"location":"tutorials/getting_started/#analyzing-multiple-metrics","title":"analyzing Multiple Metrics","text":"<p>Use <code>effdim.analyze</code> to get a report.</p> <pre><code>report = effdim.analyze(structured_data, methods=['pr', 'pca', 'shannon', 'danco'])\nprint(report)\n# {'participation_ratio': ..., 'pca': ..., 'shannon': ..., 'danco': ...}\n</code></pre>"}]}